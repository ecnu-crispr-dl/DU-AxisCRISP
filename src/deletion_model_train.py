"""
DU-AxisCRISP åˆ é™¤æ¨¡å—è®­ç»ƒè„šæœ¬
åŸºäºç¨³å®šé•¿å°¾åˆ†å¸ƒåŒåˆ†æ”¯æ¨¡å‹çš„CRISPR indelé¢„æµ‹
"""

# ============================================================================
# å¯¼å…¥ä¾èµ–
# ============================================================================
from common_def import *
from common_def import _to_tensor as _to_tensor_explicit
from models.FeatureEncoder import FeatureEncoder
import torch.nn.functional as F
from types import SimpleNamespace


# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # æ•°æ®é…ç½®
    NUM_SAMPLE = None           # ä½¿ç”¨å…¨éƒ¨æ•°æ®
    RANDOM_STATE = 42
    FEATURES = "v2"
    
    # è®­ç»ƒè¶…å‚æ•°
    EPOCHS = 700
    BATCH_SIZE = 64
    LEARING_RATE = 0.001
    
    # æŸå¤±å‡½æ•°é…ç½®
    LOSS_TYPE = "KL_Div"
    LOSS_PARAMS = {
        "alpha": 0.20,
    }
    
    # è¯„ä¼°é…ç½®
    USE_WARMUP = True
    WARMUP_EPOCHS = 15
    EVAL_FREQUENCY = 10


class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    # ç‰¹å¾ç¼–ç å™¨
    FEATURE_ENCODER_PARAMS = {
        "embedding_dim": 128,
        "output_dim": 96,
    }
    
    # é•¿å°¾åˆ†å¸ƒä¼˜åŒ–
    FREQ_THRESHOLD = 0.3
    TOP_K = 4
    WEIGHT_RATIO = 35.0
    TEMPERATURE = 0.8           # å›ºå®šæ¸©åº¦ï¼Œé¿å…æ³¢åŠ¨
    
    # ç¨³å®šæ€§å‚æ•°
    EMA_DECAY = 0.95            # å‚æ•°å¹³æ»‘ç³»æ•°
    GATE_SMOOTHING = 0.7        # é—¨æ§å¹³æ»‘ç³»æ•°
    THRESHOLD_SMOOTHING = 0.8   # é˜ˆå€¼å¹³æ»‘ç³»æ•°


# ç”Ÿæˆè¾“å‡ºæ¨¡å‹è·¯å¾„
def get_output_path(config: ModelConfig, train_config: TrainingConfig) -> str:
    """ç”Ÿæˆè¾“å‡ºæ¨¡å‹è·¯å¾„"""
    if train_config.LOSS_TYPE == "mse":
        return output_dir + f"dual_freq_v3_stable_WKL0_T{config.TEMPERATURE}_{train_config.LOSS_TYPE}_freq{config.FREQ_THRESHOLD}_{train_config.FEATURES}.pth"
    else:
        return output_dir + f"dual0_freq_v3_stable_WKL0.25_T{config.TEMPERATURE}_{train_config.LOSS_TYPE}_kl_freq{config.FREQ_THRESHOLD}_{train_config.FEATURES}_testall.pth"


# ============================================================================
# åˆå§‹åŒ–å‡½æ•°
# ============================================================================
def init_env(config: TrainingConfig):
    """åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ"""
    os.environ['OPENBLAS_NUM_THREADS'] = '1'
    os.environ['MKL_NUM_THREADS'] = '1'
    os.environ['OMP_NUM_THREADS'] = '1'
    torch.manual_seed(config.RANDOM_STATE)
    np.random.seed(config.RANDOM_STATE)
    os.makedirs(output_dir, exist_ok=True)


def get_loss_function(loss_type: str):
    """è·å–æŸå¤±å‡½æ•°"""
    if loss_type == "mse":
        return lambda y_pred, y_true: mse_loss(y_pred, y_true)
    else:
        return "KL_Div"


def init_model(num_features: int, seq_feature_dim: int, 
               model_config: ModelConfig, train_config: TrainingConfig):
    """åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    model = StableLongTailDualModel(
        num_features=num_features,
        seq_feature_dim=seq_feature_dim,
        hidden_dim=model_config.FEATURE_ENCODER_PARAMS["embedding_dim"],
        out_dim=model_config.FEATURE_ENCODER_PARAMS["output_dim"],
        temperature=model_config.TEMPERATURE,
        freq_threshold=model_config.FREQ_THRESHOLD,
        tail_alpha=0.8,
        ema_decay=model_config.EMA_DECAY,
        gate_smoothing=model_config.GATE_SMOOTHING
    ).to(DEVICE)

    optimizer = torch.optim.Adam(
        model.parameters(), 
        train_config.LEARING_RATE, 
        betas=(0.9, 0.999)
    )
    
    if train_config.USE_WARMUP:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=50, T_mult=2, eta_min=1e-6
        )
    else:
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.998)

    loss_fn = get_loss_function(train_config.LOSS_TYPE)
    return model, optimizer, scheduler, loss_fn


# ============================================================================
# æ¨¡å‹å®šä¹‰
# ============================================================================
class StableLongTailDualModel(nn.Module):
    """
    é•¿å°¾åˆ†å¸ƒåŒé¢‘æ¨¡å‹ï¼šå‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°æ³¢åŠ¨
    
    æ ¸å¿ƒè®¾è®¡ï¼š
    - å¤´éƒ¨ï¼ˆé«˜é¢‘ï¼‰ï¼šæä¿å®ˆè®¾è®¡ï¼Œä¸“æ³¨Precision
    - å°¾éƒ¨ï¼ˆä½é¢‘ï¼‰ï¼šææ•æ„Ÿè®¾è®¡ï¼Œä¸“æ³¨Recall
    - é•¿å°¾æ„ŸçŸ¥é—¨æ§ï¼šåŸºäºé¢‘ç‡åˆ†å¸ƒç‰¹å¾åŠ¨æ€è°ƒæ•´
    """
    
    def __init__(self, num_features: int, seq_feature_dim: int, 
                 hidden_dim: int = 128, out_dim: int = 96, 
                 temperature: float = 1.0, freq_threshold: float = 0.3, 
                 tail_alpha: float = 0.8, ema_decay: float = 0.95, 
                 gate_smoothing: float = 0.7):
        super(StableLongTailDualModel, self).__init__()
        
        self.temperature = temperature  
        self.freq_threshold = freq_threshold
        self.tail_alpha = tail_alpha
        self.ema_decay = ema_decay
        self.gate_smoothing = gate_smoothing

        # å…±äº«ç‰¹å¾ç¼–ç 
        self.feature_encoder = FeatureEncoder(
            num_features=num_features,
            hidden_dim=hidden_dim,
            output_dim=out_dim
        )

        # å¤´éƒ¨ï¼ˆé«˜é¢‘ï¼‰åˆ†æ”¯ï¼šæä¿å®ˆè®¾è®¡
        self.head_branch = nn.Sequential(
            nn.Linear(out_dim, out_dim),
            nn.ReLU(),
            nn.LayerNorm(out_dim),
            nn.Dropout(0.4),
            nn.Linear(out_dim, out_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.35),
            nn.Linear(out_dim // 2, 1)
        )
        
        self.head_residual = nn.Linear(out_dim, 1)
        self.head_residual_weight = nn.Parameter(torch.tensor(0.01))
        self.head_enhancer = nn.Sequential(
            nn.Linear(out_dim, out_dim // 2),
            nn.ReLU(),
            nn.Linear(out_dim // 2, 1)
        )

        # å°¾éƒ¨ï¼ˆä½é¢‘ï¼‰åˆ†æ”¯ï¼šææ•æ„Ÿè®¾è®¡
        self.tail_branch = nn.Sequential(
            nn.Linear(out_dim, out_dim),
            nn.ReLU(),
            nn.LayerNorm(out_dim),
            nn.Dropout(0.005),
            nn.Linear(out_dim, out_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.002),
            nn.Linear(out_dim // 2, 1)
        )
        
        self.tail_attention = nn.MultiheadAttention(out_dim, num_heads=2, batch_first=True)
        self.tail_attn_norm = nn.LayerNorm(out_dim)
        self.tail_enhancement = nn.Sequential(
            nn.Linear(out_dim, out_dim // 2),
            nn.ReLU(),
            nn.Linear(out_dim // 2, 1)
        )

        # é•¿å°¾æ„ŸçŸ¥é—¨æ§ï¼ˆä¿å®ˆç‰ˆï¼‰
        self.longtail_gate = nn.Sequential(
            nn.Linear(out_dim + 4, out_dim // 2),  # +4: confidence, pred_diff, freq_est, tail_est
            nn.ReLU(),
            nn.Dropout(0.1), 
            nn.Linear(out_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # è¯„ä¼°å™¨
        self.frequency_estimator = nn.Sequential(
            nn.Linear(out_dim, out_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(out_dim // 2, 1),
            nn.Sigmoid()
        )
        
        self.tail_estimator = nn.Sequential(
            nn.Linear(out_dim, out_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(out_dim // 4, 1),
            nn.Sigmoid()
        )
        
        self.confidence_estimator = nn.Sequential(
            nn.Linear(out_dim, out_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(out_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # EMAç¼“å­˜ï¼ˆç”¨äºå¹³æ»‘é—¨æ§æƒé‡ï¼‰
        self.register_buffer('gate_ema', None)
        self.log_t = nn.Parameter(torch.log(torch.tensor(1.0)))

    def current_temperature(self):
        """è·å–å½“å‰æ¸©åº¦ï¼ˆåŠ¨æ€è°ƒæ•´èŒƒå›´ï¼‰"""
        return torch.exp(self.log_t).clamp(0.1, 2.0)
    
    def _compute_head_logits(self, feat):
        """è®¡ç®—å¤´éƒ¨åˆ†æ”¯logits"""
        logit_main = self.head_branch(feat).squeeze(-1)
        logit_residual = self.head_residual(feat).squeeze(-1)
        logit_enhance = self.head_enhancer(feat).squeeze(-1)
        return logit_main + torch.sigmoid(self.head_residual_weight) * logit_residual + 0.05 * logit_enhance
    
    def _compute_tail_logits(self, feat):
        """è®¡ç®—å°¾éƒ¨åˆ†æ”¯logits"""
        attn_feat, _ = self.tail_attention(feat, feat, feat)
        attn_feat = self.tail_attn_norm(feat + attn_feat)
        logit_main = self.tail_branch(attn_feat).squeeze(-1)
        logit_enhance = self.tail_enhancement(feat).squeeze(-1)
        return logit_main + 0.6 * logit_enhance
    
    def _compute_gate(self, feat, logit_head, logit_tail):
        """è®¡ç®—é•¿å°¾æ„ŸçŸ¥é—¨æ§æƒé‡"""
        confidence = self.confidence_estimator(feat).squeeze(-1)
        frequency_est = self.frequency_estimator(feat).squeeze(-1)
        tail_est = self.tail_estimator(feat).squeeze(-1)
        pred_diff = torch.abs(logit_head - logit_tail)
        
        gate_input = torch.cat([
            feat, 
            confidence.unsqueeze(-1),
            pred_diff.unsqueeze(-1),
            frequency_est.unsqueeze(-1),
            tail_est.unsqueeze(-1)
        ], dim=-1)
        
        gate_w_raw = self.longtail_gate(gate_input).squeeze(-1)
        gate_w = 0.3 + 0.4 * gate_w_raw  # èŒƒå›´ä»[0,1]ç¼©å°åˆ°[0.3,0.7]
        
        # è®­ç»ƒæ—¶ä½¿ç”¨EMAå¹³æ»‘
        if self.training and self.gate_ema is not None:
            if self.gate_ema.shape == gate_w.shape:
                gate_w = self.ema_decay * self.gate_ema + (1 - self.ema_decay) * gate_w
            self.gate_ema = gate_w.detach()
        elif self.training:
            self.gate_ema = gate_w.detach()
        
        # é•¿å°¾æ„ŸçŸ¥èåˆ
        head_tail_weight = frequency_est * (1 - tail_est) 
        adaptive_gate = gate_w * (1 + 0.3 * head_tail_weight)  
        adaptive_gate = torch.clamp(adaptive_gate, 0.2, 0.8)
        
        return adaptive_gate, gate_w_raw, gate_w, confidence, frequency_est, tail_est, pred_diff
    
    def forward(self, x, sequences):
        """å‰å‘ä¼ æ’­"""
        feat = self.feature_encoder(x)
        logit_head = self._compute_head_logits(feat)
        logit_tail = self._compute_tail_logits(feat)
        adaptive_gate, _, _, _, _, _, _ = self._compute_gate(feat, logit_head, logit_tail)
        
        logits = adaptive_gate * logit_head + (1.0 - adaptive_gate) * logit_tail
        probs = torch.softmax(logits / (self.temperature * self.current_temperature()), dim=1)
        return probs

    def forward_with_details(self, x, sequences):
        """å‰å‘ä¼ æ’­ï¼ˆè¿”å›è¯¦ç»†ä¿¡æ¯ï¼‰"""
        feat = self.feature_encoder(x)
        logit_head = self._compute_head_logits(feat)
        logit_tail = self._compute_tail_logits(feat)
        adaptive_gate, gate_w_raw, gate_w, confidence, frequency_est, tail_est, pred_diff = \
            self._compute_gate(feat, logit_head, logit_tail)
        
        logits = adaptive_gate * logit_head + (1.0 - adaptive_gate) * logit_tail
        
        probs_fused = torch.softmax(logits / (self.temperature * self.current_temperature()), dim=1)
        probs_head = torch.softmax(logit_head / self.temperature, dim=1)
        probs_tail = torch.softmax(logit_tail / self.temperature, dim=1)
        
        return {
            "logits": logits / self.temperature,
            "probs": probs_fused,
            "probs_head": probs_head,
            "probs_tail": probs_tail,
            "gate": adaptive_gate,
            "gate_raw": gate_w_raw,
            "gate_smoothed": gate_w,
            "confidence": confidence,
            "frequency_est": frequency_est,
            "tail_est": tail_est,
            "pred_diff": pred_diff,
            "head_residual_weight": torch.sigmoid(self.head_residual_weight),
        }


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================
def build_model_config(model_config: ModelConfig, train_config: TrainingConfig,
                       X_shape: tuple, seq_shape: tuple, 
                       best_pearson: float = None, best_epoch: int = None) -> dict:
    """æ„å»ºæ¨¡å‹ä¿å­˜é…ç½®"""
    config = {
        "model_type": "stable_longtail_dual",
        "loss_type": train_config.LOSS_TYPE,
        "freq_threshold": model_config.FREQ_THRESHOLD,
        "top_k": model_config.TOP_K,
        "weight_ratio": model_config.WEIGHT_RATIO,
        "num_features": X_shape[1],
        "seq_feature_dim": seq_shape[1],
        "hidden_dim": model_config.FEATURE_ENCODER_PARAMS["embedding_dim"],
        "out_dim": model_config.FEATURE_ENCODER_PARAMS["output_dim"],
        "temperature": model_config.TEMPERATURE,
        "ema_decay": model_config.EMA_DECAY,
        "gate_smoothing": model_config.GATE_SMOOTHING,
    }
    
    if best_pearson is not None:
        config["best_pearson"] = best_pearson
    if best_epoch is not None:
        config["best_epoch"] = best_epoch
    
    return config


def save_model(model, optimizer, lr_scheduler, samples, X_shape, seq_shape, 
               model_config: ModelConfig, train_config: TrainingConfig,
               filepath: str, best_pearson: float = None, best_epoch: int = None):
    """ä¿å­˜æ¨¡å‹"""
    torch.save({
        "random_state": train_config.RANDOM_STATE,
        "model": model.state_dict(),
        "samples": samples,
        "loss_type": train_config.LOSS_TYPE,
        "optimiser": optimizer.state_dict(),
        "lr_scheduler": lr_scheduler.state_dict(),
        "feature_sets": FEATURE_SETS[train_config.FEATURES],
        "config": build_model_config(
            model_config, train_config, X_shape, seq_shape, 
            best_pearson, best_epoch
        )
    }, filepath)


# ============================================================================
# è®­ç»ƒå‡½æ•°
# ============================================================================
def train_model(X_arrays, Sequences_arrays, y_arrays, samples, 
                model, loss_fn, optimizer, lr_scheduler,
                model_config: ModelConfig, train_config: TrainingConfig,
                output_path: str, A):
    """è®­ç»ƒæ¨¡å‹ä¸»å‡½æ•°"""
    
    # æ‰“å°è®­ç»ƒé…ç½®
    print("\n" + "="*80)
    print("ğŸ”¥ Stable LongTail Dual Model: ç¨³å®šç‰ˆé•¿å°¾åˆ†å¸ƒä¼˜åŒ–")
    print("="*80)
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   - æ¨¡å‹: StableLongTailDualModel (ç¨³å®šç‰ˆé•¿å°¾åˆ†å¸ƒä¼˜åŒ–)")
    print(f"   - ç¼–ç ç»´åº¦: {model_config.FEATURE_ENCODER_PARAMS['embedding_dim']}â†’{model_config.FEATURE_ENCODER_PARAMS['output_dim']}")
    print(f"   - Temperature: {model_config.TEMPERATURE} (å›ºå®š)")
    print(f"   - KLDæƒé‡: {train_config.LOSS_PARAMS['alpha']*100:.0f}%")
    print(f"   - é«˜é¢‘é˜ˆå€¼: {model_config.FREQ_THRESHOLD}")
    print(f"   - æ ·æœ¬æ•°: {'å…¨éƒ¨' if train_config.NUM_SAMPLE is None else train_config.NUM_SAMPLE}")
    print(f"   - ç¨³å®šæ€§ä¼˜åŒ–:")
    print(f"     â€¢ EMAè¡°å‡: {model_config.EMA_DECAY}")
    print(f"     â€¢ é—¨æ§å¹³æ»‘: {model_config.GATE_SMOOTHING}")
    print(f"     â€¢ é—¨æ§èŒƒå›´: [0.2, 0.8]ï¼ˆé™åˆ¶æç«¯å€¼ï¼‰")
    print(f"     â€¢ å›ºå®šæ¸©åº¦ï¼ˆæ— è‡ªé€‚åº”æ³¢åŠ¨ï¼‰")
    print("="*80 + "\n")

    # æ•°æ®åˆ’åˆ†
    train_samples, val_samples = train_test_split(
        samples, test_size=100, random_state=train_config.RANDOM_STATE
    )
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°: {len(train_samples)}, éªŒè¯æ ·æœ¬æ•°: {len(val_samples)}")

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_X, test_y, test_samples, test_sequences = merge_tests(
        t1_path, t2_path, indel_list=ALL_INDELS, mode="union"
    )
    test_X = test_X.loc[:, FEATURE_SETS[train_config.FEATURES]]

    # åˆå§‹åŒ–è·Ÿè¸ªå˜é‡
    best_loss = float('inf')
    best_pearson = 0.0
    patience_loss = 0
    patience_pearson = 0

    # è®­ç»ƒå¾ªç¯
    for epoch in range(train_config.EPOCHS):
        epoch_start = time.time()

        # Warmupå­¦ä¹ ç‡è°ƒæ•´
        if train_config.USE_WARMUP and epoch < train_config.WARMUP_EPOCHS:
            warmup_lr = train_config.LEARING_RATE * (epoch + 1) / train_config.WARMUP_EPOCHS
            for param_group in optimizer.param_groups:
                param_group['lr'] = warmup_lr

        # æ‰“ä¹±è®­ç»ƒæ ·æœ¬
        train_samples_shuffled = np.random.permutation(train_samples)

        # æ‰¹æ¬¡è®­ç»ƒ
        epoch_losses = []
        for i in range(0, len(train_samples_shuffled), train_config.BATCH_SIZE):
            batch_samples = train_samples_shuffled[i:i+train_config.BATCH_SIZE]
            model.train()

            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            x_batch = torch.stack([_to_tensor_explicit(X_arrays.loc[s]) for s in batch_samples])
            y_batch = torch.stack([_to_tensor_explicit(y_arrays.loc[s]) for s in batch_samples])
            seq_batch = torch.stack([_to_tensor_explicit(Sequences_arrays.loc[s]) for s in batch_samples])
            y_batch = y_batch / (y_batch.sum(dim=1, keepdim=True) + 1e-10)

            # å‰å‘ä¼ æ’­
            details = model.forward_with_details(x_batch, seq_batch)
            logits_batch = details['logits']

            # è®¡ç®—æŸå¤±
            loss_total = torch.zeros(1).to(DEVICE)
            for bi in range(len(batch_samples)):
                logits = logits_batch[bi]
                y_true = y_batch[bi]

                if loss_fn == "KL_Div":
                    y_pred = torch.softmax(logits, dim=0).clamp_min(1e-12)
                    y_pred_temp = torch.softmax(
                        logits.detach() / model.current_temperature(), dim=0
                    ).clamp_min(1e-12)
                    
                    loss_wkl = weighted_kl_loss(y_pred, y_true, gamma=-0.25)
                    loss_temp = mse_loss(y_pred_temp, y_true, reduction='mean')
                    loss_total += (loss_wkl + loss_temp)
                else:
                    loss_total += loss_fn(y_pred, y_true)

            loss_total = loss_total / len(batch_samples)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss_total.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            lr_scheduler.step()

            epoch_losses.append(loss_total.detach().cpu().item())

        avg_loss = np.mean(epoch_losses) if epoch_losses else 0.0

        # è¯„ä¼°å’Œä¿å­˜
        should_eval = (epoch % train_config.EVAL_FREQUENCY == 0 and epoch >= 10) or \
                      (epoch == train_config.EPOCHS - 1)
        
        current_pearson = 0.0
        if should_eval:
            print(f"\n{'='*80}")
            print(f"Epoch {epoch}/{train_config.EPOCHS}")
            print(f"{'='*80}")
            
            print("\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°:")
            test_reg, test_cls = test_model(
                model, test_X, test_y, test_samples, test_sequences, ALL_INDELS, A
            )
            print_results(test_reg, test_cls)
            
            current_pearson = test_reg['avg_correlation']
            print(f"\nğŸ¯ å½“å‰P: {current_pearson:.4f}, æœ€ä½³P: {best_pearson:.4f}")
            print("="*80)
            
            # ä¿å­˜æœ€ä½³Pearsonæ¨¡å‹
            if current_pearson > best_pearson:
                best_pearson = current_pearson
                patience_pearson = 0
                print(f"\nğŸ‰ æ–°çš„æœ€ä½³Pearsonæ¨¡å‹! Pearson={best_pearson:.4f}")
                
                save_model(
                    model, optimizer, lr_scheduler, samples,
                    X_arrays.shape, Sequences_arrays.shape,
                    model_config, train_config,
                    output_path.replace(".pth", "_best_pearson.pth"),
                    best_pearson, epoch
                )
            else:
                patience_pearson += 1
                print(f"Pearsonæœªæ”¹å–„: {patience_pearson}/15")
        else:
            epoch_time = time.time() - epoch_start
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch:3d}: Loss={avg_loss:.6f}, LR={current_lr:.6f}, Time={epoch_time:.2f}s")
        
        # ä¿å­˜æœ€ä½³Lossæ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_loss = 0
            print(f"\nğŸ’¾ æ–°çš„æœ€ä½³Lossæ¨¡å‹! loss={best_loss:.6f}")
            
            save_model(
                model, optimizer, lr_scheduler, samples,
                X_arrays.shape, Sequences_arrays.shape,
                model_config, train_config,
                output_path.replace(".pth", "_best_loss.pth")
            )
        else:
            patience_loss += 1

        # æ—©åœæ£€æŸ¥
        if patience_loss >= 50:
            print(f"\nğŸ›‘ Lossæ—©åœ! æœ€ä½³Loss: {best_loss:.4f}")
            break

    print(f"è®­ç»ƒå®Œæˆ!")
    return best_pearson


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
if __name__ == "__main__":
    # åˆå§‹åŒ–é…ç½®
    model_config = ModelConfig()
    train_config = TrainingConfig()
    output_path = get_output_path(model_config, train_config)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("="*80)
    print("DU-AxisCRISP åˆ é™¤æ¨¡å—è®­ç»ƒ")
    print("="*80)
    print(f"é…ç½®:")
    print(f"   è®¾å¤‡: {DEVICE}")
    print(f"   æ ·æœ¬æ•°: {'å…¨éƒ¨' if train_config.NUM_SAMPLE is None else train_config.NUM_SAMPLE}")
    print(f"   ç‰¹å¾é›†: {train_config.FEATURES}")
    print(f"   Temperature: {model_config.TEMPERATURE} (å›ºå®š)")
    print(f"   å­¦ä¹ ç‡: {train_config.LEARING_RATE}")
    print(f"   Epochæ•°: {train_config.EPOCHS}")
    print(f"   Batchå¤§å°: {train_config.BATCH_SIZE}")
    print(f"   ç¨³å®šæ€§å‚æ•°:")
    print(f"     - EMAè¡°å‡: {model_config.EMA_DECAY}")
    print(f"     - é—¨æ§å¹³æ»‘: {model_config.GATE_SMOOTHING}")
    print(f"     - é˜ˆå€¼å¹³æ»‘: {model_config.THRESHOLD_SMOOTHING}")
    print("="*80 + "\n")

    # åˆå§‹åŒ–ç¯å¢ƒ
    init_env(train_config)

    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    with open(indels_sorted_path, "rb") as f:
        ALL_INDELS = pkl.load(f)

    X, y, samples, sequences = load_delete_data(
        filepath=train_file_path, 
        num_samples=train_config.NUM_SAMPLE, 
        fractions=True, 
        indel_list=ALL_INDELS
    )

    X = X.loc[:, FEATURE_SETS[train_config.FEATURES]]
    prior = compute_prior_stable(y)
    A = make_logit_adjustment(prior, tau=1, device=DEVICE)
    
    print(f"   æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   æ ·æœ¬æ•°: {len(samples)}")
    print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"   Indelæ•°: {len(ALL_INDELS)}")
    print(f"   åºåˆ—ç‰¹å¾ç»´åº¦: {sequences.shape[1]}")

    # åˆå§‹åŒ–æ¨¡å‹
    print("\nğŸ¤– åˆå§‹åŒ–æ¨¡å‹...")
    model, optimizer, lr_scheduler, loss_fn = init_model(
        X.shape[1], sequences.shape[1], model_config, train_config
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    print(f"   æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # è®­ç»ƒæ¨¡å‹
    best_pearson = train_model(
        X, sequences, y, samples, 
        model, loss_fn, optimizer, lr_scheduler,
        model_config, train_config, output_path, A
    )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    save_model(
        model, optimizer, lr_scheduler, samples,
        X.shape, sequences.shape,
        model_config, train_config,
        output_path
    )

    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    print("\n" + "="*80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("="*80)
