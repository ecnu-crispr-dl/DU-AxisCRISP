"""
DU-AxisCRISP é¢„æµ‹ç”Ÿæˆè„šæœ¬
æ•´åˆåˆ é™¤æ¨¡å‹å’Œæ’å…¥æ¨¡å‹ï¼Œç”Ÿæˆå®Œæ•´çš„indelé¢„æµ‹ç»“æœ
"""

from common_def import *
from deletion_model_train import StableLongTailDualModel
from models.tcn import AxisTCN
from ins_model_train import INSERTION_INDELS
from tqdm import tqdm
import argparse


# =============================================================================
# é…ç½®
# =============================================================================
class Config:
    """å…¨å±€é…ç½®ç±»"""
    PREDICTIONS_DIR = "./predictions/"
    MIN_NUM_READS = 100  # æ ·æœ¬è¿‡æ»¤é˜ˆå€¼
    
    # é»˜è®¤æ¨¡å‹è·¯å¾„
    DEFAULT_DEL_MODEL = "output/dual0_freq_v3_stable_WKL0.25_T0.8_KL_Div_kl_freq0.3_v2_testall_best_loss.pth"
    DEFAULT_INS_MODEL = "output/insertion_axisTCN_Sequence-only_wkl-0.1_v2_best_loss.pth"
    DEFAULT_LINDEL_MODEL = "output/100x_indel.h5"
    
    # æ•°æ®é›†é…ç½®æ˜ å°„: (åç§°, oligosæ–‡ä»¶è·¯å¾„, genotype)
    DATASET_MAPPING = {
        "test": ("test", "evaluate/predict_results/FORECast/test.fasta", "test"),
        "test2": ("LibA", "evaluate/predict_results/inDelphi/LibA.fasta", "0105-mESC-Lib1-Cas9-Tol2-BioRep2-techrep1")
    }


# =============================================================================
# æ¨¡å‹åŠ è½½
# =============================================================================
def load_deletion_model(model_path):
    checkpoint = torch.load(model_path, map_location=DEVICE, weights_only=False)
    config = checkpoint.get('config', {})
    
    model = StableLongTailDualModel(
        num_features=config.get('num_features', 6),
        seq_feature_dim=config.get('seq_feature_dim', 705),
        hidden_dim=config.get('hidden_dim', 128),
        out_dim=config.get('out_dim', 96),
        temperature=config.get('temperature', 0.8),
        freq_threshold=config.get('freq_threshold', 0.3),
        tail_alpha=0.8,
        ema_decay=config.get('ema_decay', 0.95),
        gate_smoothing=config.get('gate_smoothing', 0.7)
    ).to(DEVICE)
    
    model.load_state_dict(checkpoint['model'], strict=False)
    model.eval()
    return model


def load_insertion_model(model_path):
    checkpoint = torch.load(model_path, map_location=DEVICE, weights_only=False)
    model = AxisTCN().to(DEVICE)
    model.load_state_dict(checkpoint['model'], strict=False)
    model.eval()
    return model


def load_lindel_model(model_path):
    from tensorflow import keras
    return keras.models.load_model(model_path)


# =============================================================================
# DNAåºåˆ—ç¼–ç 
# =============================================================================
def onehotencoder(seq):
    """å•æ ¸è‹·é…¸ + åŒæ ¸è‹·é…¸ç¼–ç ï¼Œ20bp guideåºåˆ— -> 384ç»´"""
    nt = ['A', 'T', 'C', 'G']
    head = []
    l = len(seq)
    
    for k in range(l):
        for i in range(4):
            head.append(nt[i] + str(k))
    
    for k in range(l - 1):
        for i in range(4):
            for j in range(4):
                head.append(nt[i] + nt[j] + str(k))
    
    head_idx = {key: idx for idx, key in enumerate(head)}
    encode = np.zeros(len(head_idx))
    
    for j in range(l):
        encode[head_idx[seq[j] + str(j)]] = 1.
    
    for k in range(l - 1):
        encode[head_idx[seq[k:k+2] + str(k)]] = 1.
    
    return encode


def read_oligos(oligo_file_path):
    """
    è¯»å–oligoä¿¡æ¯ï¼ˆæ”¯æŒFASTAã€PKLå’ŒJSONæ ¼å¼ï¼‰
    ç”¨äºæå–guideåºåˆ—å’ŒPAMä½ç½®ä¿¡æ¯
    """
    if oligo_file_path is None:
        return None
    
    # å¤„ç†ç›¸å¯¹è·¯å¾„
    if not os.path.isabs(oligo_file_path):
        current_file = os.path.abspath(__file__)
        project_root = os.path.dirname(os.path.dirname(current_file))
        oligo_file_path = os.path.join(project_root, oligo_file_path)
    
    if not os.path.exists(oligo_file_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {oligo_file_path}")
        return None
    
    try:
        if oligo_file_path.endswith(('.pkl', '.pickle')):
            with open(oligo_file_path, 'rb') as f:
                oligos = pkl.load(f)
        elif oligo_file_path.endswith(('.fasta', '.fa')):
            # FASTAæ ¼å¼: >ID PAM_Index ORIENTATION
            #           Sequence
            oligos = []
            with open(oligo_file_path, 'r') as f:
                current_id = None
                current_pam_index = None
                for line in f:
                    line = line.strip()
                    if line.startswith('>'):
                        parts = line[1:].split()
                        if len(parts) >= 2:
                            current_id = parts[0]
                            current_pam_index = int(parts[1])
                    elif line and current_id:
                        oligos.append({
                            "ID": current_id,
                            "TargetSequence": line,
                            "PAM Index": current_pam_index
                        })
                        current_id = None
            
            if len(oligos) == 0:
                return None
        else:
            # JSONæ ¼å¼
            import json
            oligos = []
            with open(oligo_file_path, 'r') as f:
                for line in f:
                    if line.strip():
                        oligos.append(json.loads(line.strip()))
        
        return oligos
    except Exception as e:
        print(f"âš ï¸ è¯»å–oligoså¤±è´¥: {e}")
        return None


# =============================================================================
# é¢„æµ‹ç”Ÿæˆæ ¸å¿ƒé€»è¾‘
# =============================================================================
def _batch_predict_ratios(indel_model, oligos, all_samples):
    """
    æ‰¹é‡é¢„æµ‹del/insæ¯”ä¾‹ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡é¢„æµ‹æ¯”é€ä¸ªé¢„æµ‹å¿«5-10å€ï¼‰
    ä½¿ç”¨Lindelæ¨¡å‹é¢„æµ‹deletion/insertionæ¯”ä¾‹
    """
    dratio_dict = {}
    insratio_dict = {}
    valid_guides = []
    valid_sample_ids = []
    pam_dict = {'AGG', 'TGG', 'CGG', 'GGG'}
    
    for o in oligos:
        sample_id = o["ID"]
        if sample_id not in all_samples:
            continue
        
        target_seq = o["TargetSequence"]
        pam_index = o["PAM Index"]
        
        # æ ¹æ®åºåˆ—é•¿åº¦é€‰æ‹©æå–æ–¹å¼ï¼ˆä¸test_3.pyä¿æŒä¸€è‡´ï¼‰
        if len(target_seq) >= 60:
            # é•¿åºåˆ—ï¼ˆå¦‚test.fastaï¼Œ80+bpï¼‰
            seq = target_seq[pam_index-33:pam_index + 27]
            guide = seq[13:33]
            pam_seq = seq[33:36]
        else:
            # çŸ­åºåˆ—ï¼ˆå¦‚LibA.fastaï¼Œ55bpï¼‰
            guide = target_seq[pam_index-20:pam_index]
            pam_seq = target_seq[pam_index:pam_index+3]
        
        # éªŒè¯guideå’ŒPAMæœ‰æ•ˆæ€§
        if len(guide) == 20 and len(pam_seq) == 3 and pam_seq in pam_dict:
            valid_guides.append(guide)
            valid_sample_ids.append(sample_id)
        else:
            # æ— æ•ˆæ ·æœ¬ä½¿ç”¨å›ºå®šæ¯”ä¾‹
            dratio_dict[sample_id] = 0.7
            insratio_dict[sample_id] = 0.3
    
    # æ‰¹é‡é¢„æµ‹æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬
    if len(valid_guides) > 0:
        try:
            encoded_guides = np.array([onehotencoder(g) for g in valid_guides])
            batch_predictions = indel_model.predict(encoded_guides, verbose=0, batch_size=128)
            
            for i, sample_id in enumerate(valid_sample_ids):
                dratio_dict[sample_id] = batch_predictions[i, 0]
                insratio_dict[sample_id] = batch_predictions[i, 1]
        except Exception as e:
            print(f"   âš ï¸ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            dratio_dict.clear()
            insratio_dict.clear()
    
    return dratio_dict, insratio_dict


def _preextract_features(X_del, seq_full, samples):
    """
    é¢„æå–ç‰¹å¾æ•°æ®ï¼Œé¿å…å¾ªç¯å†…é¢‘ç¹ç´¢å¼•ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
    å°†DataFrameæ•°æ®æå‰è½¬æ¢ä¸ºdictï¼Œå‡å°‘é‡å¤çš„.locç´¢å¼•æ“ä½œ
    """
    X_del_dict = {}
    seq_dict = {}
    homology_dict = {}
    
    for sample_id in samples:
        # æå–v2ç‰¹å¾é›†ï¼ˆdeletionæ¨¡å‹è¾“å…¥ï¼‰
        X_del_dict[sample_id] = X_del.loc[sample_id, FEATURE_SETS["v2"]].to_numpy()
        # æå–åºåˆ—ç‰¹å¾ï¼ˆinsertionæ¨¡å‹è¾“å…¥ï¼‰
        if seq_full is not None:
            seq_dict[sample_id] = seq_full.loc[sample_id].to_numpy()
        # æå–microhomologyé•¿åº¦
        homology_dict[sample_id] = X_del.loc[sample_id, "homologyLength"]
    
    return X_del_dict, seq_dict, homology_dict


def generate_predictions(deletion_model, insertion_model, indel_model,
                        test_dataset, ALL_INDELS, output_path, oligos=None):
    """
    ç”Ÿæˆé¢„æµ‹ç»“æœ
    æ•´åˆdeletionå’Œinsertionæ¨¡å‹çš„è¾“å‡ºï¼Œç”Ÿæˆå®Œæ•´çš„indelé¢„æµ‹
    """
    print(f"\n{'='*80}")
    print(f"ğŸ”® ç”Ÿæˆé¢„æµ‹: {test_dataset}")
    print(f"{'='*80}\n")
    
    # åŠ è½½æ•°æ®ï¼ˆä¸test_3.pyä¿æŒä¸€è‡´çš„ä¸¤æ­¥åŠ è½½ç­–ç•¥ï¼‰
    data_path = test_file_path if test_dataset == 'test' else t2_path
    with open(data_path, 'rb') as f:
        data = pkl.load(f)
    y_original = data["counts"]  # åŸå§‹countsï¼Œç”¨äºè·å–å®é™…å­˜åœ¨çš„indel
    
    # åŠ è½½å¡«å……åçš„ç‰¹å¾æ•°æ®ï¼ˆç”¨äºæ¨¡å‹é¢„æµ‹ï¼‰
    X_del, y, _, seq_full = load_delete_data(
        filepath=data_path, num_samples=None, fractions=True, indel_list=ALL_INDELS
    )
    
    # æ ·æœ¬è¿‡æ»¤ï¼šä¿æŒåŸå§‹æ ·æœ¬é¡ºåº + è¿‡æ»¤ä½è¯»æ•°æ ·æœ¬
    all_samples = y_original.index.get_level_values(0).unique()
    sample_reads = y_original.groupby(level=0)["countEvents"].sum()
    common_samples = sample_reads[sample_reads >= Config.MIN_NUM_READS].index
    samples = [s for s in all_samples if s in common_samples]
    
    print(f"æ ·æœ¬æ•°: {len(samples)} (è¿‡æ»¤: min_reads >= {Config.MIN_NUM_READS})")
    print(f"åˆ é™¤ç‰¹å¾æ•°: {X_del.shape[1]}")
    print(f"åºåˆ—ç‰¹å¾ç»´åº¦: {seq_full.shape[1] if seq_full is not None else 0}")
    
    profiles = {}
    use_oligos = oligos is not None
    
    if use_oligos:
        print(f"âœ… ä½¿ç”¨oligosæå–guideåºåˆ—")
        oligos_filtered = [o for o in oligos if o["ID"] in common_samples]
        iteration_list = oligos_filtered
    else:
        print(f"âš ï¸ æœªæä¾›oligosï¼Œä½¿ç”¨å›ºå®šæ¯”ä¾‹ (0.7/0.3)")
        iteration_list = samples
    
    # æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡é¢„æµ‹del/insæ¯”ä¾‹
    if use_oligos and indel_model is not None:
        dratio_dict, insratio_dict = _batch_predict_ratios(indel_model, iteration_list, all_samples)
        print(f"   é¢„è®¡ç®—å®Œæˆï¼Œ{len(dratio_dict)} ä¸ªæ ·æœ¬")
    
    # æ€§èƒ½ä¼˜åŒ–ï¼šé¢„æå–ç‰¹å¾æ•°æ®
    print(f"\nâš¡ é¢„æå–ç‰¹å¾æ•°æ®...")
    X_del_dict, seq_dict, homology_dict = _preextract_features(X_del, seq_full, samples)
    print(f"   å®Œæˆï¼Œ{len(X_del_dict)} ä¸ªæ ·æœ¬\n")
    
    pbar = tqdm(iteration_list, desc="é¢„æµ‹è¿›åº¦", ncols=100, position=0, leave=True)
    
    for item in pbar:
        # ç¡®å®šsample_id
        if use_oligos:
            o = item
            sample_id = o["ID"]
            if sample_id not in all_samples:
                continue
        else:
            sample_id = item
        
        pbar.set_postfix_str(f"æ ·æœ¬: {sample_id[:30]}...")
        
        # æ¨¡å‹é¢„æµ‹
        with torch.no_grad():
            x_del = torch.tensor(X_del_dict[sample_id]).float().unsqueeze(0)
            
            if sample_id in seq_dict:
                seq = torch.tensor(seq_dict[sample_id]).float().unsqueeze(0)
            else:
                seq = torch.zeros(1, 705).float()
                tqdm.write(f"âš ï¸ æ ·æœ¬ {sample_id} ç¼ºå°‘åºåˆ—ç‰¹å¾")
            
            # Deletioné¢„æµ‹
            ds = deletion_model(x_del, None).squeeze(0)
            ds = (ds / ds.sum()).detach().cpu().numpy()
            
            # Insertioné¢„æµ‹
            ins = insertion_model(None, seq).squeeze(0).detach().cpu().numpy()
        
        # è·å–å®é™…å­˜åœ¨çš„deletionç±»å‹ï¼ˆå…³é”®ï¼šä¸test_3.pyä¿æŒä¸€è‡´ï¼‰
        y_obs_original = y_original.loc[sample_id]
        valid_indels = list(y_obs_original.index.intersection(ALL_INDELS))
        
        # è·å–del/insæ¯”ä¾‹ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„æ¯”ä¾‹ï¼‰
        if sample_id in dratio_dict:
            dratio, insratio = dratio_dict[sample_id], insratio_dict[sample_id]
        else:
            dratio, insratio = 0.7, 0.3
        
        # è¿‡æ»¤deletioné¢„æµ‹ï¼šåªä¿ç•™å®é™…å­˜åœ¨çš„deletionç±»å‹
        indel_index_map = {indel: i for i, indel in enumerate(ALL_INDELS)}
        ds_filtered = np.array([ds[indel_index_map[i]] for i in valid_indels])
        ds_filtered = ds_filtered / ds_filtered.sum() if ds_filtered.sum() > 0 else ds_filtered
        
        # Insertioné¢„æµ‹ï¼šä¿ç•™æ‰€æœ‰21ç§ç±»å‹ï¼ˆä¸å…¶ä»–æ¨¡å‹ä¿æŒä¸€è‡´ï¼‰
        ins_normalized = ins / ins.sum() if ins.sum() > 0 else ins
        
        # åˆå¹¶é¢„æµ‹ï¼šdeletion + insertion
        y_hat = np.concatenate((ds_filtered * dratio, ins_normalized * insratio))
        
        # æ„å»ºå®Œæ•´çš„indelåˆ—è¡¨å’ŒçœŸå®æ ‡ç­¾
        all_indels = valid_indels + list(INSERTION_INDELS)
        y_obs_selected = y_obs_original.loc[all_indels]
        y_obs_normalized = y_obs_selected["countEvents"].values / y_obs_selected["countEvents"].sum()
        
        # è®¡ç®—microhomologyæ ‡è®°
        hl = homology_dict[sample_id]
        mh = [bool(hl.get(ind, 0) > 0) for ind in all_indels]
        
        # ä¿å­˜ç»“æœ
        profiles[sample_id] = {
            "predicted": y_hat,
            "actual": y_obs_normalized,
            "indels": all_indels,
            "mh": mh
        }
    
    print(f"\nâœ… é¢„æµ‹å®Œæˆï¼Œå…± {len(profiles)} ä¸ªæ ·æœ¬")
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'wb') as f:
        pkl.dump(profiles, f)
    
    print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {output_path}")
    return profiles


# =============================================================================
# ä¸»ç¨‹åº
# =============================================================================
def main(deletion_model_path=None, insertion_model_path=None, lindel_model_path=None,
         test_dataset='test', loss_fn='KL_Div', lr='0.01'):
    """ä¸»å‡½æ•°ï¼šç”Ÿæˆé¢„æµ‹ç»“æœ"""
    
    deletion_model_path = deletion_model_path or Config.DEFAULT_DEL_MODEL
    insertion_model_path = insertion_model_path or Config.DEFAULT_INS_MODEL
    lindel_model_path = lindel_model_path or Config.DEFAULT_LINDEL_MODEL
    
    if test_dataset not in Config.DATASET_MAPPING:
        print(f"âŒ é”™è¯¯ï¼šæœªçŸ¥çš„æµ‹è¯•æ•°æ®é›† '{test_dataset}'")
        print(f"   å¯ç”¨çš„æ•°æ®é›†: {list(Config.DATASET_MAPPING.keys())}")
        return None
    
    dataset_name, oligos_file, genotype = Config.DATASET_MAPPING[test_dataset]
    
    print("=" * 80)
    print("ğŸ”® DU-AxisCRISP é¢„æµ‹ç”Ÿæˆ")
    print("=" * 80)
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"åˆ é™¤æ¨¡å‹: {deletion_model_path}")
    print(f"æ’å…¥æ¨¡å‹: {insertion_model_path}")
    print(f"Lindelæ¨¡å‹: {lindel_model_path}")
    print(f"æ•°æ®é›†: {dataset_name} (genotype: {genotype})")
    print("=" * 80 + "\n")
    
    print("ğŸ¤– åŠ è½½æ¨¡å‹...")
    deletion_model = load_deletion_model(deletion_model_path)
    insertion_model = load_insertion_model(insertion_model_path)
    
    indel_model = None
    if lindel_model_path and os.path.exists(lindel_model_path):
        indel_model = load_lindel_model(lindel_model_path)
    else:
        print("âš ï¸ æœªæä¾›Lindelæ¨¡å‹ï¼Œä½¿ç”¨å›ºå®šæ¯”ä¾‹\n")
    
    with open(indels_sorted_path, "rb") as f:
        ALL_INDELS = pkl.load(f)
    
    print(f"ğŸ“‚ åŠ è½½oligos: {oligos_file}")
    oligos = read_oligos(oligos_file)
    if not oligos:
        print(f"âŒ æ— æ³•åŠ è½½oligosæ–‡ä»¶")
        return None
    print(f"âœ… æˆåŠŸåŠ è½½ {len(oligos)} ä¸ªoligos")
    
    output_path = os.path.join(
        Config.PREDICTIONS_DIR,
        f"XCRISP_testmask_deldualmodelWKL0.25_insTCN_sequenceonly_{loss_fn}_{lr}__{genotype}.pkl"
    )
    
    profiles = generate_predictions(
        deletion_model, insertion_model, indel_model,
        test_dataset, ALL_INDELS, output_path, oligos=oligos
    )
    
    print("\n" + "=" * 80)
    print("ğŸ‰ é¢„æµ‹ç”Ÿæˆå®Œæˆ!")
    print("=" * 80)
    
    return profiles


# =============================================================================
# å‘½ä»¤è¡Œå…¥å£
# =============================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='DU-AxisCRISP é¢„æµ‹ç”Ÿæˆ')
    parser.add_argument('--deletion_model', type=str, default=None,
                        help=f'åˆ é™¤æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: {Config.DEFAULT_DEL_MODEL}ï¼‰')
    parser.add_argument('--insertion_model', type=str, default=None,
                        help=f'æ’å…¥æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: {Config.DEFAULT_INS_MODEL}ï¼‰')
    parser.add_argument('--lindel_model', type=str, default=None,
                        help=f'Lindelæ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: {Config.DEFAULT_LINDEL_MODEL}ï¼‰')
    parser.add_argument('--dataset', type=str, default='test',
                        choices=['test', 'test2'],
                        help='æµ‹è¯•æ•°æ®é›†ï¼ˆé»˜è®¤: testï¼‰')
    parser.add_argument('--loss_fn', type=str, default='KL_Div',
                        help='æŸå¤±å‡½æ•°åç§°ï¼ˆé»˜è®¤: KL_Divï¼‰')
    parser.add_argument('--lr', type=str, default='0.01',
                        help='å­¦ä¹ ç‡ï¼ˆé»˜è®¤: 0.01ï¼‰')
    
    args = parser.parse_args()
    
    main(
        deletion_model_path=args.deletion_model,
        insertion_model_path=args.insertion_model,
        lindel_model_path=args.lindel_model,
        test_dataset=args.dataset,
        loss_fn=args.loss_fn,
        lr=args.lr
    )
