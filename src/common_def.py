import os, sys
import pandas as pd
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from torch.nn.functional import kl_div, mse_loss
from sklearn.metrics import precision_score, recall_score, matthews_corrcoef, f1_score, confusion_matrix
from sklearn.model_selection import KFold, train_test_split
from tqdm import trange
import time
from scipy.stats import pearsonr
import math
import pickle as pkl
from tqdm import tqdm

#cpu/gpu win/mac
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# if DEVICE.type == 'cpu': DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu") 

# åŠ¨æ€è·¯å¾„é…ç½® - åŸºäº common_def.py æ‰€åœ¨ä½ç½®è®¡ç®—é¡¹ç›®æ ¹ç›®å½•
_CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))  # src/
_PROJECT_ROOT = os.path.dirname(_CURRENT_DIR)  # DU-AxisCRISP/

#FORECast DATA
train_file_path = os.path.join(_PROJECT_ROOT, "data", "train_new2.pkl")
test_file_path = os.path.join(_PROJECT_ROOT, "data", "test_new2.pkl")
t1_path = test_file_path  # åˆ«å

#inDelphi DATA
t2_path = os.path.join(_PROJECT_ROOT, "data", "0105-mESC-Lib1-Cas9-Tol2-BioRep2-techrep1.pkl")
indels_sorted_path = os.path.join(_PROJECT_ROOT, "data", "dele_indels_sorted.pkl")

output_dir = "output/"
FEATURE_SETS = {
    "insv1": ["Size", "Start", "leftEdge", "rightEdge", "leftEdgeMostDownstream", "rightEdgeMostUpstream", "InsSize",
              "InsGC", "InsSeqLen", "InsIsTemplated", "InsertPositionRelative", "IsPalindrome", "LeftGC", "RightGC",
              "InsSeqEncoded"],
    "full+numrepeat": ["Size", "Start", "homologyLength", "numRepeats", "homologyGCContent", "homologyDistanceRank",
                       "homologyLeftEdgeRank", "homologyRightEdgeRank", "homologyLengthRank"],
    "full": ["Size", "Start", "homologyLength", "homologyGCContent", "homologyDistanceRank", "homologyLeftEdgeRank",
             "homologyRightEdgeRank", "homologyLengthRank"],
    "v2": ["Size", "leftEdge", "rightEdge", "numRepeats", "homologyLength", "homologyGCContent"],
    "v3": ["Size", "leftEdge", "rightEdge", "homologyLength", "homologyGCContent"],
    "v4": ["Gap", "leftEdge", "rightEdge", "homologyLength", "homologyGCContent"],
    "v5": ["leftEdge", "rightEdge", "homologyLength", "homologyGCContent"],
    "v6": ["leftEdge", "Gap", "homologyLength", "homologyGCContent"],
    "v7": ["leftEdgeMostDownstream", "rightEdgeMostUpstream", "homologyLength", "homologyGCContent"],
    "ranked": ["Size", "numRepeats", "homologyLength", "homologyGCContent", "homologyLeftEdgeRank",
               "homologyRightEdgeRank"],
    "v2+ranked": ["Size", "leftEdge", "rightEdge", "numRepeats", "homologyLength", "homologyGCContent",
                  "homologyLeftEdgeRank", "homologyRightEdgeRank"]
}

def _to_tensor(arr):
    if isinstance(arr, pd.DataFrame) or isinstance(arr, pd.Series):
        arr = arr.to_numpy()
    return torch.tensor(arr).to(DEVICE).float()


def merge_tests(
    test1_path,
    test2_path,
    indel_list,
    mode="union",            # 'union' æˆ– 'intersection'
    tag=("T1","T2"),         # ç»™æ ·æœ¬åŠ åç¼€ï¼Œé¿å…åŒåå†²çª
    fractions=True,
    assert_same_seq_on_intersection=True
):
    # å¤ç”¨ä½ å·²æœ‰çš„åŠ è½½å‡½æ•°
    X1, y1, s1, seq1 = load_delete_data(test1_path, num_samples=None, fractions=True, indel_list=indel_list)
    X2, y2, s2, seq2 = load_delete_data_inDelphi(test2_path, num_samples=None, fractions=True, indel_list=indel_list)

    # å®‰å…¨æ£€æŸ¥ï¼šåˆ—ä¸€è‡´
    assert list(X1.columns) == list(X2.columns), "ç‰¹å¾åˆ—ä¸ä¸€è‡´ï¼Œè¯·å…ˆå¯¹é½"
    # MultiIndexçš„ä¸¤ä¸ªå±‚çº§åä¹Ÿåº”ä¸€è‡´
    assert X1.index.names == X2.index.names, "MultiIndex åç§°ä¸ä¸€è‡´ï¼Œè¯·å…ˆå¯¹é½"

    # å½“å‰ç´¢å¼•æ˜¯ MultiIndex: (sample, indel)
    # å…ˆæŠŠ sample å±‚æ‹¿å‡ºæ¥
    sample_level = 0  # (sample, indel) ä¸­ sample åœ¨ç¬¬0å±‚

    def add_tag_to_samples(obj, tag_str, sample_level=0):
        """
        obj: DataFrame æˆ– Seriesï¼Œç´¢å¼•ä¸º MultiIndex: (sample, indel)
        """
        import pandas as pd
        idx = obj.index
        if not isinstance(idx, pd.MultiIndex):
            raise ValueError("æœŸæœ› MultiIndex ç´¢å¼• (sample, indel)")

        # å¤åˆ¶ä¸€ä»½ levelsï¼Œä¿®æ”¹ç¬¬ sample_level å±‚åæ•´ä½“è®¾ç½®å›å»
        new_levels = list(idx.levels)
        new_levels[sample_level] = new_levels[sample_level].astype(str) + f"@{tag_str}"

        new_index = idx.set_levels(new_levels)  # æ³¨æ„ï¼šæ˜¯å¯¹ idx è°ƒç”¨ï¼Œè€Œä¸æ˜¯ df
        obj = obj.copy()
        obj.index = new_index
        return obj

    # ä¸¤ç§æ¨¡å¼ï¼šå¹¶é›† æˆ– äº¤é›†
    if mode == "union":
        X1_u = add_tag_to_samples(X1, tag[0])
        y1_u = add_tag_to_samples(y1, tag[0])
        X2_u = add_tag_to_samples(X2, tag[1])
        y2_u = add_tag_to_samples(y2, tag[1])

        X = pd.concat([X1_u, X2_u], axis=0).sort_index()
        y = pd.concat([y1_u, y2_u], axis=0).sort_index()

        # seq_features çš„è¡Œç´¢å¼•æ˜¯ sample ç»´ï¼ŒæŒ‰åŒæ ·è§„åˆ™åŠ åç¼€å†æ‹¼
        seq1_u = seq1.copy()
        seq1_u.index = seq1_u.index.astype(str) + f"@{tag[0]}"
        seq2_u = seq2.copy()
        seq2_u.index = seq2_u.index.astype(str) + f"@{tag[1]}"
        seq = pd.concat([seq1_u, seq2_u], axis=0)

        samples = list(seq.index)

    elif mode == "intersection":
        # æ‰¾åˆ°ä¸¤æµ‹è¯•é›†çš„åŒåæ ·æœ¬äº¤é›†
        common_samples = sorted(set(s1).intersection(set(s2)))
        if assert_same_seq_on_intersection and len(common_samples) > 0:
            # ç¡®è®¤äº¤é›†æ ·æœ¬çš„åºåˆ—ç‰¹å¾ä¸€è‡´
            a = seq1.loc[common_samples].sort_index()
            b = seq2.loc[common_samples].sort_index()
            if not a.equals(b):
                raise ValueError("äº¤é›†æ ·æœ¬çš„åºåˆ—ç‰¹å¾ä¸ä¸€è‡´ï¼Œè¯·ä¸è¦ç›´æ¥å¹¶ç”¨ 'intersection'ã€‚")

        # ç›´æ¥ç”¨åŒåæ ·æœ¬ï¼ˆä¸åŠ åç¼€ï¼‰ï¼Œåªä¿ç•™äº¤é›†æ ·æœ¬çš„ (sample, indel)
        idx1 = X1.index.get_level_values(0).isin(common_samples)
        idx2 = X2.index.get_level_values(0).isin(common_samples)

        X = pd.concat([X1[idx1], X2[idx2]], axis=0).sort_index()
        y = pd.concat([y1[idx1], y2[idx2]], axis=0).sort_index()
        # ä¹Ÿå¯ä»¥é€‰æ‹©â€œå–å¹³å‡/å–ä¸€ä»½â€ï¼Œè¿™é‡Œä¿ç•™ä¸¤ä»½è®°å½•ï¼ˆæ¥è‡ªä¸åŒæ¥æºï¼‰ï¼Œè¯„ä¼°æ—¶æ¯æ¡æ ·æœ¬ç‹¬ç«‹

        # åˆå¹¶äº¤é›†çš„ seq_featuresï¼ˆä¸¤ä¾§ä¸€è‡´ï¼‰
        seq = seq1.loc[common_samples].copy()
        samples = common_samples
    else:
        raise ValueError("mode åªèƒ½æ˜¯ 'union' æˆ– 'intersection'")

    # æœ€ç»ˆè¿”å›ï¼šæ‹¼æ¥åçš„ç‰¹å¾/æ ‡ç­¾/æ ·æœ¬å/åºåˆ—ç‰¹å¾
    return X, y, samples, seq

#data load
def load_delete_data_inDelphi(filepath = None, num_samples=None, fractions=True, indel_list=None):
    data = pd.read_pickle(filepath)
    counts = data["counts"]
    del_features = data["del_features"]
    seq_features = data["ins_features"]

    # ç­›é€‰æ ·æœ¬
    samples = counts.index.levels[0]
    if num_samples is not None:
        samples = samples[:num_samples]
        counts = counts.loc[samples]
        del_features = del_features.loc[samples]
    if seq_features is not None:
        # é‡è¦ï¼šé¡ºåºå¯¹é½åˆ° samples
        seq_features = seq_features.reindex(samples)

    # é€‰æ‹© DELETION æ•°æ®
    y_df = counts.loc[counts.Type == "DELETION"]
    y = y_df.fraction if fractions else y_df.countEvents

    # å¡«å…… Gap ç‰¹å¾
    del_features["Gap"] = del_features["Size"] - del_features["homologyLength"]

    # æ„å»ºå®Œæ•´çš„ MultiIndex ç´¢å¼•
    # ä½¿ç”¨æ ·æœ¬å’Œ indel_list çš„ç¬›å¡å°”ç§¯ï¼Œæ„é€ å®Œæ•´ç´¢å¼•
    index = pd.MultiIndex.from_product([samples, indel_list], names=del_features.index.names)

    # reindex del_featuresï¼Œè¡¥å…¨ç¼ºå¤±çš„ sample-indel å¯¹
    X_full = del_features.reindex(index, fill_value=0)

    # ç”Ÿæˆ maskï¼šåŸå§‹æ•°æ®å­˜åœ¨åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0
    mask = (~X_full["Size"].eq(0)).astype(float)
    X_full = X_full.copy()
    X_full["mask"] = mask

    # å¯¹ y ä¹Ÿ reindex è¡¥å…¨
    y_full = y.reindex(index, fill_value=0)
    '''with open(ALL_FEAT_PKL, "rb") as f:
        bundle = pkl.load(f)
    bio_features: pd.DataFrame = bundle["features"]  # index=Oligo_ID, columns=å…¨éƒ¨ç‰¹å¾
    # ç¼ºå¤±å·²ç»åœ¨ç”Ÿæˆ pkl æ—¶ç½® 0ï¼Œè¿™é‡Œå†ä¿é™©ä¸€æ¬¡
    bio_features = bio_features.reindex(samples).fillna(0.0)'''

    return X_full, y_full, samples, seq_features
def load_delete_data(filepath = None, num_samples=None, fractions=True, indel_list=None):
    data = pd.read_pickle(filepath)
    counts = data["counts"]
    del_features = data["del_features"]
    
    # å®‰å…¨è·å–åºåˆ—ç‰¹å¾ï¼šä¼˜å…ˆä½¿ç”¨ seq_featuresï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ ins_features
    seq_features = data.get("seq_features", None)
    if seq_features is None and "ins_features" in data:
        seq_features = data["ins_features"]  # ç¬¬äºŒä¸ªæ•°æ®é›†ä¸­ï¼Œins_features å°±æ˜¯ seq_features

    # ç­›é€‰æ ·æœ¬
    samples = counts.index.levels[0]
    if num_samples is not None:
        samples = samples[:num_samples]
        counts = counts.loc[samples]
        del_features = del_features.loc[samples]
    if seq_features is not None:
        # é‡è¦ï¼šé¡ºåºå¯¹é½åˆ° samples
        seq_features = seq_features.reindex(samples)

    # é€‰æ‹© DELETION æ•°æ®
    y_df = counts.loc[counts.Type == "DELETION"]
    y = y_df.fraction if fractions else y_df.countEvents

    # å¡«å…… Gap ç‰¹å¾
    del_features["Gap"] = del_features["Size"] - del_features["homologyLength"]

    # æ„å»ºå®Œæ•´çš„ MultiIndex ç´¢å¼•
    # ä½¿ç”¨æ ·æœ¬å’Œ indel_list çš„ç¬›å¡å°”ç§¯ï¼Œæ„é€ å®Œæ•´ç´¢å¼•
    index = pd.MultiIndex.from_product([samples, indel_list], names=del_features.index.names)

    # reindex del_featuresï¼Œè¡¥å…¨ç¼ºå¤±çš„ sample-indel å¯¹
    X_full = del_features.reindex(index, fill_value=0)

    # ç”Ÿæˆ maskï¼šåŸå§‹æ•°æ®å­˜åœ¨åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0
    mask = (~X_full["Size"].eq(0)).astype(float)
    X_full = X_full.copy()
    X_full["mask"] = mask

    # å¯¹ y ä¹Ÿ reindex è¡¥å…¨
    y_full = y.reindex(index, fill_value=0)
    '''with open(ALL_FEAT_PKL, "rb") as f:
        bundle = pkl.load(f)
    bio_features: pd.DataFrame = bundle["features"]  # index=Oligo_ID, columns=å…¨éƒ¨ç‰¹å¾
    # ç¼ºå¤±å·²ç»åœ¨ç”Ÿæˆ pkl æ—¶ç½® 0ï¼Œè¿™é‡Œå†ä¿é™©ä¸€æ¬¡
    bio_features = bio_features.reindex(samples).fillna(0.0)'''

    return X_full, y_full, samples, seq_features
def load_insert_data(filepath = None, num_samples=None, fractions=True, indel_list=None):
    data = pd.read_pickle(filepath)
    counts = data["counts"]
    ins_features = data["ins_features"]
    seq_features = data.get("seq_features", None)  # å®‰å…¨è·å–ï¼Œå¯èƒ½ä¸å­˜åœ¨

    # ç­›é€‰æ ·æœ¬
    samples = counts.index.levels[0]
    if num_samples is not None:
        samples = samples[:num_samples]
        counts = counts.loc[samples]
        ins_features = ins_features.loc[samples]
    if seq_features is not None:
        # é‡è¦ï¼šé¡ºåºå¯¹é½åˆ° samples
        seq_features = seq_features.reindex(samples)

    # é€‰æ‹© DELETION æ•°æ®
    y_df = counts.loc[counts.Type == "INSERTION"]
    y = y_df.fraction if fractions else y_df.countEvents

    # å¡«å…… Gap ç‰¹å¾
    ins_features["Gap"] = ins_features["Size"] - ins_features["homologyLength"]

    # æ„å»ºå®Œæ•´çš„ MultiIndex ç´¢å¼•
    # ä½¿ç”¨æ ·æœ¬å’Œ indel_list çš„ç¬›å¡å°”ç§¯ï¼Œæ„é€ å®Œæ•´ç´¢å¼•
    index = pd.MultiIndex.from_product([samples, indel_list], names=ins_features.index.names)

    # reindex del_featuresï¼Œè¡¥å…¨ç¼ºå¤±çš„ sample-indel å¯¹
    X_full = ins_features.reindex(index, fill_value=0)

    # ç”Ÿæˆ maskï¼šåŸå§‹æ•°æ®å­˜åœ¨åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0
    mask = (~X_full["Size"].eq(0)).astype(float)
    X_full = X_full.copy()
    X_full["mask"] = mask

    # å¯¹ y ä¹Ÿ reindex è¡¥å…¨
    y_full = y.reindex(index, fill_value=0)
    '''with open(ALL_FEAT_PKL, "rb") as f:
        bundle = pkl.load(f)
    bio_features: pd.DataFrame = bundle["features"]  # index=Oligo_ID, columns=å…¨éƒ¨ç‰¹å¾
    # ç¼ºå¤±å·²ç»åœ¨ç”Ÿæˆ pkl æ—¶ç½® 0ï¼Œè¿™é‡Œå†ä¿é™©ä¸€æ¬¡
    bio_features = bio_features.reindex(samples).fillna(0.0)'''
    #first_6_nt_feature_indices = list(range(0, 56)) + list(range(80, 304))
    return X_full, y_full, samples, seq_features#.iloc[:,first_6_nt_feature_indices]
def load_insdel_data(filepath = None, num_samples=None, fractions=True):
    data = pd.read_pickle(filepath)
    counts = data["counts"]
    ins_features = data["ins_features"]
    seq_features = data["seq_features"]

    # ç­›é€‰æ ·æœ¬
    samples = counts.index.levels[0]
    if num_samples is not None:
        samples = samples[:num_samples]
        counts = counts.loc[samples]
        ins_features = ins_features.loc[samples]
    if seq_features is not None:
        # é‡è¦ï¼šé¡ºåºå¯¹é½åˆ° samples
        seq_features = seq_features.reindex(samples)
    # é€‰æ‹©åº¦é‡
    val_col = "fraction" if fractions else "countEvents"
    # åˆ†å®¶æ—èšåˆåˆ°æ ·æœ¬çº§
    ins_sum = counts[counts["Type"] == "INSERTION"].groupby(level=0)[val_col].sum()
    del_sum = counts[counts["Type"] == "DELETION"].groupby(level=0)[val_col].sum()

    # å¡«å…… Gap ç‰¹å¾
    ins_features["Gap"] = ins_features["Size"] - ins_features["homologyLength"]

    # æ„å»ºå®Œæ•´çš„ MultiIndex ç´¢å¼•
    # ä½¿ç”¨æ ·æœ¬å’Œ indel_list çš„ç¬›å¡å°”ç§¯ï¼Œæ„é€ å®Œæ•´ç´¢å¼•
    #index = pd.MultiIndex.from_product([samples, indel_list], names=ins_features.index.names)

    # reindex del_featuresï¼Œè¡¥å…¨ç¼ºå¤±çš„ sample-indel å¯¹
    X_full = ins_features#.reindex(index, fill_value=0)

    # ç”Ÿæˆ maskï¼šåŸå§‹æ•°æ®å­˜åœ¨åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0
    mask = (~X_full["Size"].eq(0)).astype(float)
    X_full = X_full.copy()
    X_full["mask"] = mask

    total = (ins_sum + del_sum).replace(0, np.finfo(float).eps)
    y_ins = (ins_sum / total).rename("INS")
    y_del = (del_sum / total).rename("DEL")
    y2_df = pd.concat([y_ins, y_del], axis=1)

    # æ•°å€¼ç¨³å®šï¼ˆä¿è¯å’Œä¸º1ï¼‰
    y2_df["INS"] = y2_df["INS"].clip(0, 1)
    y2_df["DEL"] = 1.0 - y2_df["INS"]
    '''with open(ALL_FEAT_PKL, "rb") as f:
        bundle = pkl.load(f)
    bio_features: pd.DataFrame = bundle["features"]  # index=Oligo_ID, columns=å…¨éƒ¨ç‰¹å¾
    # ç¼ºå¤±å·²ç»åœ¨ç”Ÿæˆ pkl æ—¶ç½® 0ï¼Œè¿™é‡Œå†ä¿é™©ä¸€æ¬¡
    bio_features = bio_features.reindex(samples).fillna(0.0)'''
    #first_6_nt_feature_indices = list(range(0, 56)) + list(range(80, 304))
    return X_full, y2_df, samples, seq_features#.iloc[:,first_6_nt_feature_indices]

def load_delete_data_include_dnaFeatures(filepath = None, dna_feature_path = None, num_samples=None, fractions=True, indel_list=None):
    data = pd.read_pickle(filepath)
    counts = data["counts"]
    del_features = data["del_features"]
    seq_features = data["seq_features"]    

    # ç­›é€‰æ ·æœ¬
    samples = counts.index.levels[0]
    if num_samples is not None:
        samples = samples[:num_samples]
        counts = counts.loc[samples]
        del_features = del_features.loc[samples]
    
    # âœ… ä»dna_feature_pathè¯»å–DNA features
    if dna_feature_path is not None:
        # è¯»å–DNA featuresæ–‡ä»¶
        dna_features = pd.read_pickle(dna_feature_path)
        # DNA featuresçš„ç´¢å¼•åº”è¯¥æ˜¯Oligo_IDï¼Œéœ€è¦å¯¹é½åˆ°samples
        seq_features = dna_features.reindex(samples, fill_value=0.0)
    elif seq_features is not None:
        # åŸæœ‰é€»è¾‘ï¼šä»dataä¸­è¯»å–çš„seq_features
        # é‡è¦ï¼šé¡ºåºå¯¹é½åˆ° samples
        seq_features = seq_features.reindex(samples)

    # é€‰æ‹© DELETION æ•°æ®
    y_df = counts.loc[counts.Type == "DELETION"]
    y = y_df.fraction if fractions else y_df.countEvents

    # å¡«å…… Gap ç‰¹å¾
    del_features["Gap"] = del_features["Size"] - del_features["homologyLength"]

    # æ„å»ºå®Œæ•´çš„ MultiIndex ç´¢å¼•
    # ä½¿ç”¨æ ·æœ¬å’Œ indel_list çš„ç¬›å¡å°”ç§¯ï¼Œæ„é€ å®Œæ•´ç´¢å¼•
    index = pd.MultiIndex.from_product([samples, indel_list], names=del_features.index.names)

    # reindex del_featuresï¼Œè¡¥å…¨ç¼ºå¤±çš„ sample-indel å¯¹
    X_full = del_features.reindex(index, fill_value=0)

    # ç”Ÿæˆ maskï¼šåŸå§‹æ•°æ®å­˜åœ¨åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0
    mask = (~X_full["Size"].eq(0)).astype(float)
    X_full = X_full.copy()
    X_full["mask"] = mask

    # å¯¹ y ä¹Ÿ reindex è¡¥å…¨
    y_full = y.reindex(index, fill_value=0)
    '''with open(ALL_FEAT_PKL, "rb") as f:
        bundle = pkl.load(f)
    bio_features: pd.DataFrame = bundle["features"]  # index=Oligo_ID, columns=å…¨éƒ¨ç‰¹å¾
    # ç¼ºå¤±å·²ç»åœ¨ç”Ÿæˆ pkl æ—¶ç½® 0ï¼Œè¿™é‡Œå†ä¿é™©ä¸€æ¬¡
    bio_features = bio_features.reindex(samples).fillna(0.0)'''

    return X_full, y_full, samples, seq_features


def batch_model(X, seq_full, Y, samples, model, loss_fn, optimizer, lr_scheduler):
    model.train()
    loss = torch.zeros(1).to(DEVICE)

    # æ„é€  batch è¾“å…¥ (B, N, F)
    x_batch = torch.stack([_to_tensor(X.loc[s]) for s in samples])  # (batch, num_indels, num_features)
    y_batch = torch.stack([_to_tensor(Y.loc[s]) for s in samples])  # (batch, num_indels)
    seq_batch = torch.stack([_to_tensor(seq_full.loc[s]) for s in samples])

    y_batch = y_batch / (y_batch.sum(dim=1, keepdim=True) + 1e-10)

    # Forward pass
    y_pred_batch= model(x_batch, seq_batch)


    # é€ä¸ªæ ·æœ¬è®¡ç®— loss å¹¶åŠ å’Œ
    for i in range(len(samples)):
        y_pred = y_pred_batch[i]
        y_true = y_batch[i]

        if loss_fn == "KL_Div":
            # PyTorchçš„kl_divéœ€è¦ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯logæ¦‚ç‡
            y_pred_clamped = torch.clamp(y_pred, min=1e-8, max=1-1e-8)
            loss += kl_div(torch.log(y_pred_clamped), y_true, reduction='batchmean')
        else:
            loss += loss_fn(y_pred, y_true)

    loss = torch.div(loss, len(samples))  # loss = loss / len(samples)

    # Backprop
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    lr_scheduler.step()

    return loss.cpu().detach().numpy().item()


def analyze_sample_zeros(test_y):
    sample_stats = test_y.groupby(level='Sample_Name').agg({
        'total_indels': 'size',
        'zero_indels': lambda x: (x == 0).sum(),
        'non_zero_indels': lambda x: (x != 0).sum(),
        'all_zeros': lambda x: np.all(x == 0),
        'max_fraction': 'max',
        'min_fraction': 'min'
    })

    # é‡å‘½ååˆ—
    sample_stats = sample_stats.rename(columns={
        'total_indels': 'æ€»Indelæ•°',
        'zero_indels': 'é›¶å€¼Indelæ•°',
        'non_zero_indels': 'éé›¶Indelæ•°',
        'all_zeros': 'æ˜¯å¦å…¨ä¸ºé›¶',
        'max_fraction': 'æœ€å¤§é¢‘ç‡',
        'min_fraction': 'æœ€å°é¢‘ç‡'
    })

    return sample_stats

def test_model(model, X, Y, samples, seq_features, indel_list,A):
    """
    æµ‹è¯•æ¨¡å‹æ€§èƒ½
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        X: ç‰¹å¾æ•°æ®
        Y: æ ‡ç­¾æ•°æ®
        samples: æ ·æœ¬åˆ—è¡¨
        seq_features: åºåˆ—ç‰¹å¾
        indel_list: indelåˆ—è¡¨
    
    Returns:
        regression_metrics, classification_metrics: å›å½’å’Œåˆ†ç±»æŒ‡æ ‡
    """
    model.eval()
    
    # å›å½’æŒ‡æ ‡
    correlations = []
    kl_divergences = []
    mse=[]

    # åˆ†ç±»æŒ‡æ ‡æ•°æ®
    y_true_list = []
    y_pred_list = []
    THRESHOLDS = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]

    with torch.no_grad():
        for i in range(len(samples)):
            sample = samples[i]

            x_data = X.loc[sample]
            y_data = Y.loc[sample]    

            x = _to_tensor(x_data)  
            y = _to_tensor(y_data)  
                
            # æ·»åŠ batchç»´åº¦
            x = x.unsqueeze(0)  # (1, indel_num, feature_num)
                
            # å¤„ç†åºåˆ—ç‰¹å¾
            if seq_features is not None:
                seq_data = seq_features.loc[sample]
                xs = _to_tensor(seq_data).unsqueeze(0)  # (1, seq_len, 4)
            else:
                xs = torch.zeros(1, 79, 4).to(DEVICE)  # é»˜è®¤åºåˆ—ç‰¹å¾
                
            # æ¨¡å‹é¢„æµ‹
            y_pred = model(x, xs)  # (1, indel_num)
            #y_pred = torch.softmax((y_pred) , dim=1)
            y_pred = y_pred.squeeze(0)  # (indel_num,)

            ''' head_logits, tail_logits, mask_logits, embout = model(x, xs, None,
                                                                  mask=None)  # .squeeze(-1)  # (batch, num_indels)
            mask_hard = (torch.sigmoid(mask_logits) > 0.3)
            fusion, weight_head = model.ensemble(embout.detach(), head_logits.detach(), tail_logits.detach(),
                                                 mask_hard.detach(), A)'''

            #head_prob = torch.softmax((head_logits), dim=1)
            #y_pred=head_prob.squeeze(0)
            #print(tail_prob)
            
            # æ£€æŸ¥é¢„æµ‹ç»“æœ
            if torch.isnan(y_pred).any() or torch.isinf(y_pred).any():
                print(f"âš ï¸  æ ·æœ¬ {sample} é¢„æµ‹ç»“æœåŒ…å«NaNæˆ–Inf")
                continue
                
            # å½’ä¸€åŒ–
            y_sum = y.sum()
            y_pred_sum = y_pred.sum()
            
            if y_sum <= 1e-10:
                y_sum+=1e-10
                #print(f"âš ï¸  æ ·æœ¬ {sample} æ ‡ç­¾å’Œä¸ºé›¶")
                #continue
                
            if y_pred_sum <= 1e-10:
                y_pred_sum+=1e-10
                #print(f"âš ï¸  æ ·æœ¬ {sample} é¢„æµ‹å’Œä¸ºé›¶")
                #continue
                
            y = y / y_sum
            y_pred = y_pred / y_pred_sum
            
            # è½¬æ¢ä¸ºnumpy
            y_np = y.cpu().numpy()
            y_pred_np = y_pred.cpu().numpy()
            
            # æœ€ç»ˆæ£€æŸ¥
            if np.isnan(y_np).any() or np.isnan(y_pred_np).any():
                print(f"âš ï¸  æ ·æœ¬ {sample} å½’ä¸€åŒ–ååŒ…å«NaN")
                continue
            
            # è®¡ç®—å›å½’æŒ‡æ ‡    
            if len(y_np) > 1 and np.var(y_np) > 0:
                corr = pearsonr(y_pred_np, y_np)[0]
                if not np.isnan(corr):
                    correlations.append(corr)
                
                y_pred_clamped = torch.clamp(y_pred, min=1e-8, max=1-1e-8)
                kl_value = kl_div(torch.log(y_pred_clamped), y, reduction='batchmean').cpu().item()
                mse_value=mse_loss(y_pred_clamped, y, reduction='mean').cpu().item()
                if not np.isnan(kl_value) and not np.isinf(kl_value) and kl_value >= 0:
                    kl_divergences.append(kl_value)
                mse.append(mse_value)
                
            # ä¿å­˜ç”¨äºåˆ†ç±»æŒ‡æ ‡è®¡ç®—
            y_true_list.append(y_np)
            y_pred_list.append(y_pred_np)
                
    
    # è®¡ç®—å›å½’æŒ‡æ ‡
    regression_metrics = {
        'avg_correlation': np.mean(correlations) if correlations else 0.0,
        'avg_kl_divergence': np.mean(kl_divergences) if kl_divergences else float('inf'),
        'avg_mse': np.mean(mse) if mse else float('inf'),
        'num_samples': len(correlations)
    }
    #print("å½“å‰å…¨å±€æ¸©åº¦ï¼š",model.current_temperature())
    # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
    classification_metrics = calculate_classification_metrics(y_true_list, y_pred_list, THRESHOLDS)
    
    return regression_metrics, classification_metrics

def print_results(regression_metrics, classification_metrics):
    """
    æ‰“å°æµ‹è¯•ç»“æœ
    
    Args:
        regression_metrics: å›å½’æŒ‡æ ‡
        classification_metrics: åˆ†ç±»æŒ‡æ ‡
    """
    print(f"\n" + "="*80)
    print(f"ğŸ“Š æ¨¡å‹æµ‹è¯•ç»“æœ")
    print(f"="*80)
    
    # å›å½’æŒ‡æ ‡
    print(f"\nğŸ” å›å½’æŒ‡æ ‡:")
    print(f"   - å¹³å‡Pearsonç›¸å…³ç³»æ•°: {regression_metrics['avg_correlation']:.4f}")
    print(f"   - å¹³å‡KLæ•£åº¦: {regression_metrics['avg_kl_divergence']:.6f}")
    print(f"   - å¹³å‡mseæŸå¤±: {regression_metrics['avg_mse']:.6f}")
    print(f"   - æœ‰æ•ˆæ ·æœ¬æ•°: {regression_metrics['num_samples']}")
    
    # åˆ†ç±»æŒ‡æ ‡
    print(f"\nğŸ¯ åˆ†ç±»æŒ‡æ ‡ (å„é˜ˆå€¼è¯¦ç»†ç»“æœ):")
    print(f"-"*80)
    for i, threshold in enumerate(classification_metrics['thresholds']):
        tp = classification_metrics['tp'][i]
        fp = classification_metrics['fp'][i]
        tn = classification_metrics['tn'][i]
        fn = classification_metrics['fn'][i]
        total = tp + fp + tn + fn
        
        print(f"é˜ˆå€¼ {threshold:.1f}: "
              f"Precision={classification_metrics['precision'][i]:.4f}, "
              f"Recall={classification_metrics['recall'][i]:.4f}, "
              f"MCC={classification_metrics['mcc'][i]:.4f}, "
              f"F1={classification_metrics['f1_score'][i]:.4f}")
        print(f"         TP={tp}, FP={fp}, TN={tn}, FN={fn} | æ€»æ ·æœ¬={total}")
        print()
    
    # æ€»ä½“ç»Ÿè®¡
    total_tp = sum(classification_metrics['tp'])
    total_fp = sum(classification_metrics['fp'])
    total_tn = sum(classification_metrics['tn'])
    total_fn = sum(classification_metrics['fn'])
    
    print(f"ğŸ“ˆ æ€»ä½“åˆ†ç±»ç»Ÿè®¡:")
    print(f"   - æ€»TP: {total_tp}")
    print(f"   - æ€»FP: {total_fp}")
    print(f"   - æ€»TN: {total_tn}")
    print(f"   - æ€»FN: {total_fn}")
    
    # å¹³å‡æŒ‡æ ‡
    avg_precision = np.mean(classification_metrics['precision'])
    avg_recall = np.mean(classification_metrics['recall'])
    avg_mcc = np.mean(classification_metrics['mcc'])
    avg_f1 = np.mean(classification_metrics['f1_score'])
    
    print(f"\nğŸ“Š å¹³å‡åˆ†ç±»æŒ‡æ ‡:")
    print(f"   - å¹³å‡Precision: {avg_precision:.4f}")
    print(f"   - å¹³å‡Recall: {avg_recall:.4f}")
    print(f"   - å¹³å‡MCC: {avg_mcc:.4f}")
    print(f"   - å¹³å‡F1-Score: {avg_f1:.4f}")
    
    print(f"\n" + "="*80)


def calculate_classification_metrics(y_true_list, y_pred_list, thresholds):
    """
    è®¡ç®—åˆ†ç±»æŒ‡æ ‡
    
    Args:
        y_true_list: çœŸå®æ ‡ç­¾åˆ—è¡¨
        y_pred_list: é¢„æµ‹æ¦‚ç‡åˆ—è¡¨
        thresholds: é˜ˆå€¼åˆ—è¡¨
    
    Returns:
        metrics_dict: åŒ…å«å„é˜ˆå€¼ä¸‹æŒ‡æ ‡çš„å­—å…¸
    """
    results = {
        'thresholds': thresholds,
        'precision': [], 'recall': [], 'mcc': [], 'f1_score': [],
        'tp': [], 'fp': [], 'tn': [], 'fn': []
    }
    
    print(f"\nğŸ“Š è®¡ç®—åˆ†ç±»æŒ‡æ ‡ (æ ·æœ¬æ•°: {len(y_true_list)})")
    
    for threshold in thresholds:
        threshold_preds = []
        threshold_labels = []
        
        for y_true, y_pred in zip(y_true_list, y_pred_list):
            true_label = 1 if np.sum(y_true > threshold) == 1 else 0
            
            # é¢„æµ‹æ ‡ç­¾ï¼šæ˜¯å¦é¢„æµ‹æœ‰åˆ é™¤å‘ç”Ÿ
            pred_label = 1 if np.sum(y_pred > threshold) == 1 else 0
            
            threshold_labels.append(true_label)
            threshold_preds.append(pred_label)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        if len(set(threshold_labels)) > 1:  # ç¡®ä¿æœ‰ä¸¤ä¸ªç±»åˆ«
            tn, fp, fn, tp = confusion_matrix(threshold_labels, threshold_preds).ravel()
            
            # è®¡ç®—æŒ‡æ ‡
            prec = precision_score(threshold_labels, threshold_preds, zero_division=0)
            rec = recall_score(threshold_labels, threshold_preds, zero_division=0)
            mcc = matthews_corrcoef(threshold_labels, threshold_preds)
            f1 = f1_score(threshold_labels, threshold_preds, zero_division=0)
            
            results['precision'].append(prec)
            results['recall'].append(rec)
            results['mcc'].append(mcc)
            results['f1_score'].append(f1)
            results['tp'].append(int(tp))
            results['fp'].append(int(fp))
            results['tn'].append(int(tn))
            results['fn'].append(int(fn))
        else:
            # åªæœ‰ä¸€ä¸ªç±»åˆ«çš„æƒ…å†µ
            results['precision'].append(0.0)
            results['recall'].append(0.0)
            results['mcc'].append(0.0)
            results['f1_score'].append(0.0)
            results['tp'].append(0)
            results['fp'].append(0)
            results['tn'].append(len(threshold_labels))
            results['fn'].append(0)
    
    return results
def compute_prior_stable(
    y_full: pd.Series,
    indel_list=None,
    alpha: float = 1.0,         # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
    lam: float = 0.05,          # ä¸å‡åŒ€åˆ†å¸ƒæ’å€¼ç³»æ•°
    pi_min: float = 1e-10,       # åœ°æ¿ï¼Œé¿å…è¿‡å°
    pi_max: float = 0.4         # å¤©èŠ±æ¿ï¼Œé¿å…è¿‡å¤§ï¼ˆå¯é€‚å½“æ”¾å®½ï¼‰
):
    # 1) åªç»Ÿè®¡çœŸå®å­˜åœ¨çš„ç±»ï¼ˆæ±‚æ¯ä¸ª indel çš„å…¨å±€è®¡æ•°/è´¨é‡ï¼‰
    y = y_full.clip(lower=0)
    c = y.groupby(level=1).sum()   # Series: indel -> mass

    # å¯¹é½é¡ºåº
    if indel_list is not None:
        c = c.reindex(indel_list, fill_value=0.0)

    # 2) æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
    c = c.to_numpy(dtype=np.float64)
    c_smooth = c + alpha

    # 3) å½’ä¸€åŒ– + ä¸å‡åŒ€åˆ†å¸ƒæ’å€¼
    pi = c_smooth / c_smooth.sum()
    K = len(pi)
    pi = (1 - lam) * pi + lam * (1.0 / K)

    # 4) è£å‰ªé¿å…æç«¯
    pi = np.clip(pi, pi_min, pi_max)
    pi = pi / pi.sum()

    return pi.astype(np.float32)
def make_logit_adjustment(pi: np.ndarray, tau: float = 1.0, device: str = "cpu"):
    # A = -tau * log(pi)
    A = tau * np.log(np.clip(pi, 1e-12, 1.0)).astype(np.float32)
    return torch.tensor(A, dtype=torch.float32, device=device)
def weighted_kl_loss(
    pred: torch.Tensor,   # (B, C)
    p_true: torch.Tensor,   # (B, C) éè´Ÿï¼Œå¯æœªå½’ä¸€åŒ–
    gamma: float = 0.5,     # å¤´éƒ¨æ”¾å¤§å¼ºåº¦: 0.5~2 å¸¸ç”¨
    mask: torch.Tensor = None,  # (B, C) å¯é€‰æ— æ•ˆç±»æ©ç 
    eps: float = 1e-8
) -> torch.Tensor:
    """
    ä»…å¤´éƒ¨æ”¾å¤§ p^gamma çš„åŠ æƒ KLï¼ˆæ— é˜ˆå€¼çª—/ç„¦ç‚¹/é—´éš”é¡¹ï¼‰ã€‚
    è¿”å›æ ‡é‡ lossã€‚
    """
    #q = F.softmax(logits, dim=-1)
    p = p_true.clamp_min(0.0)
    p = p / p.sum(dim=-1, keepdim=True).clamp_min(eps)

    # å¤´éƒ¨æ”¾å¤§æƒé‡ w = p^gamma
    w = (p + eps) ** gamma

    # åŠ æƒ forward-KL
    kl_elem = w * p * (torch.log(p + eps) - torch.log(pred + eps))

    if mask is not None:
        m = mask.to(dtype=kl_elem.dtype)
        kl_elem = kl_elem * m
        valid = m.sum(dim=-1).clamp_min(1.0)
        loss = kl_elem.sum(dim=-1) / valid
        return loss.mean()

    return kl_elem.mean()


def continuous_weighted_kl_loss(pred, p_true, gamma_func="linear",eps: float = 1e-8):
    """
    è¿ç»­å˜åŒ–çš„gammaå€¼
    """
    p = p_true.clamp_min(0.0)
    p = p / p.sum(dim=-1, keepdim=True).clamp_min(eps)

    if gamma_func == "linear":
        # gammaä»-0.1çº¿æ€§å˜åŒ–åˆ°-0.5
        gamma = -0.25 - 0.1 * p  # pè¶Šå¤§ï¼Œgammaè¶Šè´Ÿ
    elif gamma_func == "sigmoid":
        # Så½¢å˜åŒ–
        gamma = -0.3 - 0.4 * torch.sigmoid(10 * (p - 0.05))
    elif gamma_func == "piecewise_linear":
        # åˆ†æ®µçº¿æ€§
        gamma = torch.where(
            p < 0.01,
            -0.1,
            torch.where(
                p < 0.05,
                -0.1 - 2.0 * (p - 0.01),  # å¿«é€Ÿå˜åŒ–
                -0.3 - 0.5 * (p - 0.05)  # æ…¢é€Ÿå˜åŒ–
            )
        )

    weights = (p + eps) ** gamma
    kl_elem = weights * p * (torch.log(p + eps) - torch.log(pred + eps))
    return kl_elem.mean()


def adaptive_gamma_weighted_kl(pred, p_true, base_gamma=-0.3, eps=1e-8):
    """
    æ ¹æ®å½“å‰æ¨¡å‹è¡¨ç°åŠ¨æ€è°ƒæ•´gamma
    """
    # è®¡ç®—æ¨¡å‹åœ¨å½“å‰æ•°æ®ä¸Šçš„å¹³å‡ç½®ä¿¡åº¦ - éœ€è¦detachï¼
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦ - predå·²ç»æ˜¯æ¦‚ç‡
    with torch.no_grad():
        avg_confidence = pred.max(dim=-1)[0].mean()

    # å¦‚æœæ¨¡å‹æ•´ä½“ç½®ä¿¡åº¦é«˜ï¼Œæ›´å…³æ³¨å°¾éƒ¨ï¼›ç½®ä¿¡åº¦ä½ï¼Œæ›´å…³æ³¨å¤´éƒ¨
    dynamic_gamma = base_gamma + 0.3 * (avg_confidence - 0.5)

    w = (p_true + eps) ** dynamic_gamma
    kl_elem = w * p_true * (torch.log(p_true + eps) - torch.log(pred + eps))
    return kl_elem.mean()


def curriculum_weighted_kl(pred, p_true, epoch, total_epochs=200, base_gamma=0,eps=1e-8):
    """
    éšç€è®­ç»ƒè¿›è¡Œï¼Œé€æ­¥å¢åŠ å¯¹å°¾éƒ¨çš„å…³æ³¨
    """
    # çº¿æ€§è¯¾ç¨‹ï¼šä»å…³æ³¨å¤´éƒ¨é€æ­¥è½¬ç§»åˆ°å…³æ³¨å°¾éƒ¨
    progress = min(epoch / total_epochs,1)
    curriculum_gamma = base_gamma - 0.4 * progress  # ä»-0.3é€æ­¥åˆ°-0.7

    w = (p_true + eps) ** curriculum_gamma
    kl_elem = w * p_true * (torch.log(p_true + eps) - torch.log(pred + eps))
    return kl_elem.mean()