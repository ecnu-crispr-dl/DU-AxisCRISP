import torch
import torch.nn as nn
import torch.nn.functional as F
from .tabkanet import NumEncoderTransformer


class TabKANFeatureEncoder(nn.Module):
    """
    ğŸ”¥ TabKANç‰¹å¾ç¼–ç å™¨ - ä½¿ç”¨NumEncoderTransformer
    å‚è€ƒdeletion_3.pyçš„å®ç°
    """
    def __init__(self, num_features, embedding_dim=64, output_dim=64):
        super(TabKANFeatureEncoder, self).__init__()
        
        self.num_features = num_features
        self.embedding_dim = embedding_dim
        
        # NumEncoderTransformer: å°†num_featuresç¼–ç æˆembedding_dim
        self.num_encoder = NumEncoderTransformer(num_features, embedding_dim)
        
        '''# åå¤„ç†å±‚
        self.post_encoder = nn.Sequential(
            nn.LayerNorm(num_features * embedding_dim),
            nn.Dropout(0.1),
            nn.Linear(num_features * embedding_dim, embedding_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim * 2, output_dim)
        )'''
    
    def forward(self, x):
        """
        Args:
            x: (B, N, num_features) - batch, num_indels, features
        Returns:
            features: (B, N, output_dim)
        """
        B, N, F = x.shape
        
        # å±•å¹³å¤„ç†: (B, N, F) â†’ (B*N, F)
        x_flat = x.view(B * N, F)
        
        # NumEncoderTransformerç¼–ç 
        # (B*N, F) â†’ (B*N, F*embedding_dim)
        x_encoded = self.num_encoder(x_flat)
        
        # åå¤„ç†
        # (B*N, F*embedding_dim) â†’ (B*N, output_dim)
        #features = self.post_encoder(x_encoded)
        
        # æ¢å¤å½¢çŠ¶: (B*N, output_dim) â†’ (B, N, output_dim)
        features = x_encoded.view(B, N, -1)
        
        return features
