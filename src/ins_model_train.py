"""
DU-AxisCRISP æ’å…¥æ¨¡å—è®­ç»ƒè„šæœ¬
åŸºäºAxisTCNæ¨¡å‹çš„CRISPRæ’å…¥é¢„æµ‹
"""

# ============================================================================
# å¯¼å…¥ä¾èµ–
# ============================================================================
from common_def import *
from common_def import _to_tensor as _to_tensor_explicit
from models.tcn import AxisTCN
import torch.nn.functional as F


# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # æ•°æ®é…ç½®
    NUM_SAMPLE = None           # ä½¿ç”¨å…¨éƒ¨æ•°æ®
    RANDOM_STATE = 42
    FEATURES = "insv1"
    
    # è®­ç»ƒè¶…å‚æ•°
    EPOCHS = 41
    BATCH_SIZE = 64
    LEARING_RATE = 0.0002
    
    # è®­ç»ƒç­–ç•¥
    USE_WARMUP = True
    WARMUP_EPOCHS = 15
    EVAL_FREQUENCY = 10


class LossConfig:
    """æŸå¤±å‡½æ•°é…ç½®"""
    LOSS_TYPE = "weighted_kl"
    ALPHA = -0.1  # weighted KL lossçš„gammaå‚æ•°


# æ’å…¥indelåˆ—è¡¨ï¼ˆ21ç±»ï¼‰
INSERTION_INDELS = [
    '1+A', '1+T', '1+C', '1+G',
    '2+AA', '2+AT', '2+AC', '2+AG', '2+TA', '2+TT', '2+TC', '2+TG',
    '2+CA', '2+CT', '2+CC', '2+CG', '2+GA', '2+GT', '2+GC', '2+GG',
    '3+X'
]


# ============================================================================
# è·¯å¾„å’Œè¾“å‡º
# ============================================================================
def get_output_path(config: LossConfig) -> str:
    """ç”Ÿæˆè¾“å‡ºæ¨¡å‹è·¯å¾„"""
    return output_dir + f"insertion_axisTCN_Sequence-only_wkl{config.ALPHA}_v2.pth"


# ============================================================================
# åˆå§‹åŒ–å‡½æ•°
# ============================================================================
def init_env(config: TrainingConfig):
    """åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ"""
    os.environ['OPENBLAS_NUM_THREADS'] = '1'
    os.environ['MKL_NUM_THREADS'] = '1'
    os.environ['OMP_NUM_THREADS'] = '1'
    torch.manual_seed(config.RANDOM_STATE)
    np.random.seed(config.RANDOM_STATE)
    os.makedirs(output_dir, exist_ok=True)


def init_model(train_config: TrainingConfig):
    """åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    model = AxisTCN().to(DEVICE)
    
    optimizer = torch.optim.Adam(
        model.parameters(), 
        train_config.LEARING_RATE, 
        betas=(0.9, 0.999)
    )
    
    if train_config.USE_WARMUP:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=50, T_mult=2, eta_min=1e-6
        )
    else:
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.998)
    
    return model, optimizer, scheduler


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================
def build_model_config(train_config: TrainingConfig, loss_config: LossConfig,
                       X_shape: tuple, seq_shape: tuple, 
                       best_pearson: float = None, best_epoch: int = None) -> dict:
    """æ„å»ºæ¨¡å‹ä¿å­˜é…ç½®"""
    config = {
        "model_type": "insertion_axisTCN",
        "loss_type": loss_config.LOSS_TYPE,
        "alpha": loss_config.ALPHA,
        "num_features": X_shape[1],
        "seq_feature_dim": seq_shape[1],
    }
    
    if best_pearson is not None:
        config["best_pearson"] = best_pearson
    if best_epoch is not None:
        config["best_epoch"] = best_epoch
    
    return config


def save_model(model, optimizer, lr_scheduler, samples, X_shape, seq_shape,
               train_config: TrainingConfig, loss_config: LossConfig,
               filepath: str, best_pearson: float = None, best_epoch: int = None):
    """ä¿å­˜æ¨¡å‹"""
    torch.save({
        "random_state": train_config.RANDOM_STATE,
        "model": model.state_dict(),
        "samples": samples,
        "loss_type": loss_config.LOSS_TYPE,
        "optimiser": optimizer.state_dict(),
        "lr_scheduler": lr_scheduler.state_dict(),
        "feature_sets": FEATURE_SETS[train_config.FEATURES],
        "config": build_model_config(
            train_config, loss_config, X_shape, seq_shape,
            best_pearson, best_epoch
        )
    }, filepath)


# ============================================================================
# è®­ç»ƒå‡½æ•°
# ============================================================================
def train_model(X_arrays, Sequences_arrays, y_arrays, samples,
                model, optimizer, lr_scheduler,
                train_config: TrainingConfig, loss_config: LossConfig,
                output_path: str, A):
    """è®­ç»ƒæ¨¡å‹ä¸»å‡½æ•°"""
    
    # æ‰“å°è®­ç»ƒé…ç½®
    print("\n" + "=" * 80)
    print("ğŸ”¥ Insertion Model Training: AxisTCN Sequence-only")
    print("=" * 80)
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   - æ¨¡å‹: AxisTCN (åºåˆ—ç‰¹å¾ä¸“ç”¨)")
    print(f"   - Loss: weighted_kl_loss (alpha={loss_config.ALPHA})")
    print(f"   - æ ·æœ¬æ•°: {'å…¨éƒ¨' if train_config.NUM_SAMPLE is None else train_config.NUM_SAMPLE}")
    print(f"   - å­¦ä¹ ç‡: {train_config.LEARING_RATE}")
    print(f"   - Batchå¤§å°: {train_config.BATCH_SIZE}")
    print("=" * 80 + "\n")

    train_samples, val_samples = train_test_split(samples, test_size=100, random_state=train_config.RANDOM_STATE)

    test_X, test_y, test_samples, test_sequences = load_insert_data(
        filepath=t1_path, num_samples=None, fractions=True, indel_list=INSERTION_INDELS
    )
    test_X = test_X.loc[:, FEATURE_SETS[train_config.FEATURES]]


    # åˆå§‹åŒ–è·Ÿè¸ªå˜é‡
    best_loss = float('inf')
    best_pearson = 0.0
    patience_loss = 0
    patience_pearson = 0

    # è®­ç»ƒå¾ªç¯
    for epoch in range(train_config.EPOCHS):
        epoch_start = time.time()

        # Warmupå­¦ä¹ ç‡è°ƒæ•´
        if train_config.USE_WARMUP and epoch < train_config.WARMUP_EPOCHS:
            warmup_lr = train_config.LEARING_RATE * (epoch + 1) / train_config.WARMUP_EPOCHS
            for param_group in optimizer.param_groups:
                param_group['lr'] = warmup_lr

        # æ‰“ä¹±è®­ç»ƒæ ·æœ¬
        train_samples_shuffled = np.random.permutation(train_samples)

        # æ‰¹æ¬¡è®­ç»ƒ
        epoch_losses = []
        for i in range(0, len(train_samples_shuffled), train_config.BATCH_SIZE):
            batch_samples = train_samples_shuffled[i:i + train_config.BATCH_SIZE]
            model.train()

            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            y_batch = torch.stack([_to_tensor_explicit(y_arrays.loc[s]) for s in batch_samples])
            seq_batch = torch.stack([_to_tensor_explicit(Sequences_arrays.loc[s]) for s in batch_samples])
            y_batch = y_batch / (y_batch.sum(dim=1, keepdim=True) + 1e-10)

            # å‰å‘ä¼ æ’­ï¼ˆAxisTCNåªä½¿ç”¨åºåˆ—ç‰¹å¾ï¼‰
            y_pred_batch = model(None, seq_batch)

            # è®¡ç®—æŸå¤±
            loss_total = torch.zeros(1).to(DEVICE)
            for bi in range(len(batch_samples)):
                y_true = y_batch[bi]
                y_pred = y_pred_batch[bi]
                loss_wkl = weighted_kl_loss(y_pred, y_true, gamma=loss_config.ALPHA)
                loss_total += loss_wkl

            loss_total = loss_total / len(batch_samples)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss_total.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            lr_scheduler.step()

            epoch_losses.append(loss_total.detach().cpu().item())

        avg_loss = np.mean(epoch_losses) if epoch_losses else 0.0

        # è¯„ä¼°å’Œä¿å­˜
        should_eval = (epoch % train_config.EVAL_FREQUENCY == 0 and epoch >= 30) or \
                      (epoch == train_config.EPOCHS - 1)
        
        current_pearson = 0.0
        if should_eval:
            print(f"\n{'=' * 80}")
            print(f"Epoch {epoch}/{train_config.EPOCHS}")
            print(f"{'=' * 80}")
            
            print("\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°:")
            test_reg, test_cls = test_model(
                model, test_X, test_y, test_samples, test_sequences, INSERTION_INDELS, A
            )
            print_results(test_reg, test_cls)
            
            current_pearson = test_reg['avg_correlation']
            print(f"\nğŸ¯ å½“å‰Pearson: {current_pearson:.4f}, æœ€ä½³Pearson: {best_pearson:.4f}")
            print("=" * 80)
            
            # ä¿å­˜æœ€ä½³Pearsonæ¨¡å‹
            if current_pearson > best_pearson:
                best_pearson = current_pearson
                patience_pearson = 0
                print(f"\nğŸ‰ æ–°çš„æœ€ä½³Pearsonæ¨¡å‹! Pearson={best_pearson:.4f}")
                
                save_model(
                    model, optimizer, lr_scheduler, samples,
                    X_arrays.shape, Sequences_arrays.shape,
                    train_config, loss_config,
                    output_path.replace(".pth", "_best_pearson.pth"),
                    best_pearson, epoch
                )
            else:
                patience_pearson += 1
                print(f"Pearsonæœªæ”¹å–„: {patience_pearson}/15")
        else:
            epoch_time = time.time() - epoch_start
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch:3d}: Loss={avg_loss:.6f}, LR={current_lr:.6f}, Time={epoch_time:.2f}s")
        
        # ä¿å­˜æœ€ä½³Lossæ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_loss = 0
            print(f"\nğŸ’¾ æ–°çš„æœ€ä½³Lossæ¨¡å‹! loss={best_loss:.6f}")
            
            save_model(
                model, optimizer, lr_scheduler, samples,
                X_arrays.shape, Sequences_arrays.shape,
                train_config, loss_config,
                output_path.replace(".pth", "_best_loss.pth")
            )
        else:
            patience_loss += 1

        # æ—©åœæ£€æŸ¥
        if patience_loss >= 30:
            print(f"\nğŸ›‘ Lossæ—©åœ! æœ€ä½³Loss: {best_loss:.6f}")
            break

    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ¯ æœ€ä½³Pearson: {best_pearson:.4f}")
    return best_pearson


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
if __name__ == "__main__":
    # åˆå§‹åŒ–é…ç½®
    train_config = TrainingConfig()
    loss_config = LossConfig()
    output_path = get_output_path(loss_config)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("=" * 80)
    print("DU-AxisCRISP æ’å…¥æ¨¡å—è®­ç»ƒ")
    print("=" * 80)
    print(f"é…ç½®:")
    print(f"   è®¾å¤‡: {DEVICE}")
    print(f"   æ ·æœ¬æ•°: {'å…¨éƒ¨' if train_config.NUM_SAMPLE is None else train_config.NUM_SAMPLE}")
    print(f"   ç‰¹å¾é›†: {train_config.FEATURES}")
    print(f"   å­¦ä¹ ç‡: {train_config.LEARING_RATE}")
    print(f"   Epochæ•°: {train_config.EPOCHS}")
    print(f"   Batchå¤§å°: {train_config.BATCH_SIZE}")
    print(f"   Loss Alpha: {loss_config.ALPHA}")
    print("=" * 80 + "\n")

    # åˆå§‹åŒ–ç¯å¢ƒ
    init_env(train_config)

    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    X, y, samples, sequences = load_insert_data(
        filepath=train_file_path,
        num_samples=train_config.NUM_SAMPLE,
        fractions=True,
        indel_list=INSERTION_INDELS
    )
    X = X.loc[:, FEATURE_SETS[train_config.FEATURES]]
    prior = compute_prior_stable(y)
    A = make_logit_adjustment(prior, tau=1, device=DEVICE)
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   æ ·æœ¬æ•°: {len(samples)}")
    print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"   Indelæ•°: {len(INSERTION_INDELS)}")
    print(f"   åºåˆ—ç‰¹å¾ç»´åº¦: {sequences.shape[1]}")

    # åˆå§‹åŒ–æ¨¡å‹
    print("\nğŸ¤– åˆå§‹åŒ–æ¨¡å‹...")
    model, optimizer, lr_scheduler = init_model(train_config)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    print(f"   æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # è®­ç»ƒæ¨¡å‹
    best_pearson = train_model(
        X, sequences, y, samples,
        model, optimizer, lr_scheduler,
        train_config, loss_config, output_path, A
    )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    save_model(
        model, optimizer, lr_scheduler, samples,
        X.shape, sequences.shape,
        train_config, loss_config,
        output_path
    )

    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("=" * 80)
